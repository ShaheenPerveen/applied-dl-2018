save path : ./logs/
{'batch_size': 32, 'data_path': 'd:/db/data/seedlings/train/', 'dataset': 'seedlings', 'decay': 0.0005, 'epochs': 300, 'evaluate': False, 'gammas': [0.1, 0.1], 'imgDim': 3, 'img_scale': 224, 'learning_rate': 0.0002, 'manualSeed': 999, 'momentum': 0.9, 'ngpu': 1, 'num_classes': 12, 'print_freq': 200, 'resume': '', 'save_path': './logs/', 'save_path_model': './logs//seedlings/SimpleNet/', 'schedule': [150, 225], 'start_epoch': 0, 'tensorboard': True, 'test_data_path': 'd:/db/data/seedlings/test/', 'use_cuda': True, 'validationRatio': 0.85, 'workers': 0}

==>>[2018-03-17 14:35:15] [Epoch=000/300] [Need: 00:00:00] [learning_rate=0.0002] [Best : Accuracy=0.00, Error=100.00]

==>>Epoch=[000/300]], [2018-03-17 14:35:15], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [000][000/127]   Time 1.477 (1.477)   Data 0.254 (0.254)   Loss 2.4876 (2.4876)   Prec@1 12.500 (12.500)   Prec@5 37.500 (37.500)   [2018-03-17 14:35:16]
  **Train** Prec@1 26.523 Prec@5 72.462 Error@1 73.477
  **VAL** Prec@1 39.888 Prec@5 86.096 Error@1 60.112

==>>[2018-03-17 14:36:12] [Epoch=001/300] [Need: 04:36:47] [learning_rate=0.0002] [Best : Accuracy=39.89, Error=60.11]

==>>Epoch=[001/300]], [2018-03-17 14:36:12], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [001][000/127]   Time 0.470 (0.470)   Data 0.394 (0.394)   Loss 2.1918 (2.1918)   Prec@1 25.000 (25.000)   Prec@5 75.000 (75.000)   [2018-03-17 14:36:12]
  **Train** Prec@1 33.829 Prec@5 82.021 Error@1 66.171
  **VAL** Prec@1 47.472 Prec@5 94.803 Error@1 52.528

==>>[2018-03-17 14:37:08] [Epoch=002/300] [Need: 04:36:14] [learning_rate=0.0002] [Best : Accuracy=47.47, Error=52.53]

==>>Epoch=[002/300]], [2018-03-17 14:37:08], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [002][000/127]   Time 0.280 (0.280)   Data 0.244 (0.244)   Loss 1.9527 (1.9527)   Prec@1 25.000 (25.000)   Prec@5 81.250 (81.250)   [2018-03-17 14:37:08]
  **Train** Prec@1 40.713 Prec@5 85.983 Error@1 59.287
  **VAL** Prec@1 40.449 Prec@5 88.062 Error@1 59.551

==>>[2018-03-17 14:38:03] [Epoch=003/300] [Need: 04:35:22] [learning_rate=0.0002] [Best : Accuracy=47.47, Error=52.53]

==>>Epoch=[003/300]], [2018-03-17 14:38:03], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [003][000/127]   Time 0.351 (0.351)   Data 0.314 (0.314)   Loss 1.7042 (1.7042)   Prec@1 43.750 (43.750)   Prec@5 78.125 (78.125)   [2018-03-17 14:38:04]
  **Train** Prec@1 43.437 Prec@5 87.667 Error@1 56.563
  **VAL** Prec@1 54.213 Prec@5 92.416 Error@1 45.787

==>>[2018-03-17 14:38:59] [Epoch=004/300] [Need: 04:34:36] [learning_rate=0.0002] [Best : Accuracy=54.21, Error=45.79]

==>>Epoch=[004/300]], [2018-03-17 14:38:59], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [004][000/127]   Time 0.389 (0.389)   Data 0.353 (0.353)   Loss 1.5243 (1.5243)   Prec@1 46.875 (46.875)   Prec@5 84.375 (84.375)   [2018-03-17 14:38:59]
  **Train** Prec@1 45.666 Prec@5 89.747 Error@1 54.334
  **VAL** Prec@1 63.343 Prec@5 96.348 Error@1 36.657

==>>[2018-03-17 14:39:55] [Epoch=005/300] [Need: 04:33:38] [learning_rate=0.0002] [Best : Accuracy=63.34, Error=36.66]

==>>Epoch=[005/300]], [2018-03-17 14:39:55], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [005][000/127]   Time 0.304 (0.304)   Data 0.267 (0.267)   Loss 1.4694 (1.4694)   Prec@1 40.625 (40.625)   Prec@5 93.750 (93.750)   [2018-03-17 14:39:55]
  **Train** Prec@1 47.945 Prec@5 90.936 Error@1 52.055
  **VAL** Prec@1 62.500 Prec@5 96.910 Error@1 37.500

==>>[2018-03-17 14:40:50] [Epoch=006/300] [Need: 04:32:46] [learning_rate=0.0002] [Best : Accuracy=63.34, Error=36.66]

==>>Epoch=[006/300]], [2018-03-17 14:40:50], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [006][000/127]   Time 0.330 (0.330)   Data 0.293 (0.293)   Loss 1.7055 (1.7055)   Prec@1 31.250 (31.250)   Prec@5 84.375 (84.375)   [2018-03-17 14:40:51]
  **Train** Prec@1 49.183 Prec@5 90.094 Error@1 50.817
  **VAL** Prec@1 61.798 Prec@5 94.242 Error@1 38.202

==>>[2018-03-17 14:41:46] [Epoch=007/300] [Need: 04:31:55] [learning_rate=0.0002] [Best : Accuracy=63.34, Error=36.66]

==>>Epoch=[007/300]], [2018-03-17 14:41:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [007][000/127]   Time 0.481 (0.481)   Data 0.444 (0.444)   Loss 1.4849 (1.4849)   Prec@1 53.125 (53.125)   Prec@5 96.875 (96.875)   [2018-03-17 14:41:47]
  **Train** Prec@1 51.263 Prec@5 91.877 Error@1 48.737
  **VAL** Prec@1 52.669 Prec@5 92.416 Error@1 47.331

==>>[2018-03-17 14:42:42] [Epoch=008/300] [Need: 04:31:00] [learning_rate=0.0002] [Best : Accuracy=63.34, Error=36.66]

==>>Epoch=[008/300]], [2018-03-17 14:42:42], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [008][000/127]   Time 0.316 (0.316)   Data 0.280 (0.280)   Loss 1.2477 (1.2477)   Prec@1 56.250 (56.250)   Prec@5 96.875 (96.875)   [2018-03-17 14:42:42]
  **Train** Prec@1 51.436 Prec@5 91.852 Error@1 48.564
  **VAL** Prec@1 63.904 Prec@5 96.770 Error@1 36.096

==>>[2018-03-17 14:43:37] [Epoch=009/300] [Need: 04:30:04] [learning_rate=0.0002] [Best : Accuracy=63.90, Error=36.10]

==>>Epoch=[009/300]], [2018-03-17 14:43:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [009][000/127]   Time 0.353 (0.353)   Data 0.316 (0.316)   Loss 1.3117 (1.3117)   Prec@1 37.500 (37.500)   Prec@5 90.625 (90.625)   [2018-03-17 14:43:38]
  **Train** Prec@1 54.507 Prec@5 92.125 Error@1 45.493
  **VAL** Prec@1 64.326 Prec@5 96.348 Error@1 35.674

==>>[2018-03-17 14:44:33] [Epoch=010/300] [Need: 04:29:08] [learning_rate=0.0002] [Best : Accuracy=64.33, Error=35.67]

==>>Epoch=[010/300]], [2018-03-17 14:44:33], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [010][000/127]   Time 0.299 (0.299)   Data 0.263 (0.263)   Loss 1.6575 (1.6575)   Prec@1 40.625 (40.625)   Prec@5 84.375 (84.375)   [2018-03-17 14:44:33]
  **Train** Prec@1 54.780 Prec@5 93.190 Error@1 45.220
  **VAL** Prec@1 72.331 Prec@5 97.472 Error@1 27.669

==>>[2018-03-17 14:45:29] [Epoch=011/300] [Need: 04:28:28] [learning_rate=0.0002] [Best : Accuracy=72.33, Error=27.67]

==>>Epoch=[011/300]], [2018-03-17 14:45:29], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [011][000/127]   Time 0.339 (0.339)   Data 0.301 (0.301)   Loss 1.3664 (1.3664)   Prec@1 50.000 (50.000)   Prec@5 93.750 (93.750)   [2018-03-17 14:45:30]
  **Train** Prec@1 55.844 Prec@5 93.710 Error@1 44.156
  **VAL** Prec@1 73.596 Prec@5 98.174 Error@1 26.404

==>>[2018-03-17 14:46:25] [Epoch=012/300] [Need: 04:27:37] [learning_rate=0.0002] [Best : Accuracy=73.60, Error=26.40]

==>>Epoch=[012/300]], [2018-03-17 14:46:25], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [012][000/127]   Time 0.440 (0.440)   Data 0.404 (0.404)   Loss 1.4202 (1.4202)   Prec@1 59.375 (59.375)   Prec@5 93.750 (93.750)   [2018-03-17 14:46:26]
  **Train** Prec@1 58.668 Prec@5 94.106 Error@1 41.332
  **VAL** Prec@1 77.528 Prec@5 98.315 Error@1 22.472

==>>[2018-03-17 14:47:21] [Epoch=013/300] [Need: 04:26:40] [learning_rate=0.0002] [Best : Accuracy=77.53, Error=22.47]

==>>Epoch=[013/300]], [2018-03-17 14:47:21], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [013][000/127]   Time 0.362 (0.362)   Data 0.325 (0.325)   Loss 1.1822 (1.1822)   Prec@1 53.125 (53.125)   Prec@5 100.000 (100.000)   [2018-03-17 14:47:21]
  **Train** Prec@1 57.528 Prec@5 93.561 Error@1 42.472
  **VAL** Prec@1 77.107 Prec@5 97.893 Error@1 22.893

==>>[2018-03-17 14:48:17] [Epoch=014/300] [Need: 04:25:40] [learning_rate=0.0002] [Best : Accuracy=77.53, Error=22.47]

==>>Epoch=[014/300]], [2018-03-17 14:48:17], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [014][000/127]   Time 0.440 (0.440)   Data 0.403 (0.403)   Loss 1.3137 (1.3137)   Prec@1 50.000 (50.000)   Prec@5 96.875 (96.875)   [2018-03-17 14:48:17]
  **Train** Prec@1 59.411 Prec@5 94.527 Error@1 40.589
  **VAL** Prec@1 70.084 Prec@5 97.051 Error@1 29.916

==>>[2018-03-17 14:49:12] [Epoch=015/300] [Need: 04:24:45] [learning_rate=0.0002] [Best : Accuracy=77.53, Error=22.47]

==>>Epoch=[015/300]], [2018-03-17 14:49:12], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [015][000/127]   Time 0.366 (0.366)   Data 0.329 (0.329)   Loss 1.0052 (1.0052)   Prec@1 71.875 (71.875)   Prec@5 93.750 (93.750)   [2018-03-17 14:49:13]
  **Train** Prec@1 59.782 Prec@5 94.799 Error@1 40.218
  **VAL** Prec@1 80.758 Prec@5 98.034 Error@1 19.242

==>>[2018-03-17 14:50:08] [Epoch=016/300] [Need: 04:23:45] [learning_rate=0.0002] [Best : Accuracy=80.76, Error=19.24]

==>>Epoch=[016/300]], [2018-03-17 14:50:08], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [016][000/127]   Time 0.248 (0.248)   Data 0.211 (0.211)   Loss 0.9423 (0.9423)   Prec@1 71.875 (71.875)   Prec@5 100.000 (100.000)   [2018-03-17 14:50:08]
  **Train** Prec@1 60.054 Prec@5 94.156 Error@1 39.946
  **VAL** Prec@1 63.062 Prec@5 96.348 Error@1 36.938

==>>[2018-03-17 14:51:04] [Epoch=017/300] [Need: 04:22:48] [learning_rate=0.0002] [Best : Accuracy=80.76, Error=19.24]

==>>Epoch=[017/300]], [2018-03-17 14:51:04], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [017][000/127]   Time 0.330 (0.330)   Data 0.290 (0.290)   Loss 1.1623 (1.1623)   Prec@1 68.750 (68.750)   Prec@5 96.875 (96.875)   [2018-03-17 14:51:04]
  **Train** Prec@1 61.763 Prec@5 94.775 Error@1 38.237
  **VAL** Prec@1 77.669 Prec@5 98.876 Error@1 22.331

==>>[2018-03-17 14:51:59] [Epoch=018/300] [Need: 04:21:51] [learning_rate=0.0002] [Best : Accuracy=80.76, Error=19.24]

==>>Epoch=[018/300]], [2018-03-17 14:51:59], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [018][000/127]   Time 0.372 (0.372)   Data 0.337 (0.337)   Loss 1.0846 (1.0846)   Prec@1 62.500 (62.500)   Prec@5 100.000 (100.000)   [2018-03-17 14:52:00]
  **Train** Prec@1 61.169 Prec@5 94.601 Error@1 38.831
  **VAL** Prec@1 80.337 Prec@5 99.157 Error@1 19.663

==>>[2018-03-17 14:52:55] [Epoch=019/300] [Need: 04:20:57] [learning_rate=0.0002] [Best : Accuracy=80.76, Error=19.24]

==>>Epoch=[019/300]], [2018-03-17 14:52:55], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [019][000/127]   Time 0.363 (0.363)   Data 0.327 (0.327)   Loss 0.9080 (0.9080)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 14:52:55]
  **Train** Prec@1 61.491 Prec@5 95.171 Error@1 38.509
  **VAL** Prec@1 71.348 Prec@5 99.017 Error@1 28.652

==>>[2018-03-17 14:53:51] [Epoch=020/300] [Need: 04:20:02] [learning_rate=0.0002] [Best : Accuracy=80.76, Error=19.24]

==>>Epoch=[020/300]], [2018-03-17 14:53:51], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [020][000/127]   Time 0.402 (0.402)   Data 0.365 (0.365)   Loss 1.0526 (1.0526)   Prec@1 65.625 (65.625)   Prec@5 96.875 (96.875)   [2018-03-17 14:53:51]
  **Train** Prec@1 64.438 Prec@5 95.518 Error@1 35.562
  **VAL** Prec@1 70.646 Prec@5 97.893 Error@1 29.354

==>>[2018-03-17 14:54:46] [Epoch=021/300] [Need: 04:19:04] [learning_rate=0.0002] [Best : Accuracy=80.76, Error=19.24]

==>>Epoch=[021/300]], [2018-03-17 14:54:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [021][000/127]   Time 0.279 (0.279)   Data 0.243 (0.243)   Loss 1.2789 (1.2789)   Prec@1 62.500 (62.500)   Prec@5 90.625 (90.625)   [2018-03-17 14:54:47]
  **Train** Prec@1 61.615 Prec@5 95.617 Error@1 38.385
  **VAL** Prec@1 82.303 Prec@5 99.157 Error@1 17.697

==>>[2018-03-17 14:55:42] [Epoch=022/300] [Need: 04:18:08] [learning_rate=0.0002] [Best : Accuracy=82.30, Error=17.70]

==>>Epoch=[022/300]], [2018-03-17 14:55:42], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [022][000/127]   Time 0.271 (0.271)   Data 0.236 (0.236)   Loss 0.9590 (0.9590)   Prec@1 68.750 (68.750)   Prec@5 100.000 (100.000)   [2018-03-17 14:55:42]
  **Train** Prec@1 64.289 Prec@5 95.641 Error@1 35.711
  **VAL** Prec@1 76.966 Prec@5 98.736 Error@1 23.034

==>>[2018-03-17 14:56:38] [Epoch=023/300] [Need: 04:17:11] [learning_rate=0.0002] [Best : Accuracy=82.30, Error=17.70]

==>>Epoch=[023/300]], [2018-03-17 14:56:38], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [023][000/127]   Time 0.291 (0.291)   Data 0.255 (0.255)   Loss 0.9467 (0.9467)   Prec@1 62.500 (62.500)   Prec@5 100.000 (100.000)   [2018-03-17 14:56:38]
  **Train** Prec@1 64.537 Prec@5 95.988 Error@1 35.463
  **VAL** Prec@1 80.197 Prec@5 99.438 Error@1 19.803

==>>[2018-03-17 14:57:33] [Epoch=024/300] [Need: 04:16:16] [learning_rate=0.0002] [Best : Accuracy=82.30, Error=17.70]

==>>Epoch=[024/300]], [2018-03-17 14:57:33], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [024][000/127]   Time 0.511 (0.511)   Data 0.472 (0.472)   Loss 1.1889 (1.1889)   Prec@1 56.250 (56.250)   Prec@5 96.875 (96.875)   [2018-03-17 14:57:34]
  **Train** Prec@1 65.255 Prec@5 95.815 Error@1 34.745
  **VAL** Prec@1 78.230 Prec@5 98.034 Error@1 21.770

==>>[2018-03-17 14:58:29] [Epoch=025/300] [Need: 04:15:19] [learning_rate=0.0002] [Best : Accuracy=82.30, Error=17.70]

==>>Epoch=[025/300]], [2018-03-17 14:58:29], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [025][000/127]   Time 0.344 (0.344)   Data 0.308 (0.308)   Loss 0.9940 (0.9940)   Prec@1 56.250 (56.250)   Prec@5 100.000 (100.000)   [2018-03-17 14:58:29]
  **Train** Prec@1 65.503 Prec@5 95.963 Error@1 34.497
  **VAL** Prec@1 85.112 Prec@5 99.157 Error@1 14.888

==>>[2018-03-17 14:59:25] [Epoch=026/300] [Need: 04:14:23] [learning_rate=0.0002] [Best : Accuracy=85.11, Error=14.89]

==>>Epoch=[026/300]], [2018-03-17 14:59:25], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [026][000/127]   Time 0.378 (0.378)   Data 0.341 (0.341)   Loss 0.8133 (0.8133)   Prec@1 71.875 (71.875)   Prec@5 96.875 (96.875)   [2018-03-17 14:59:25]
  **Train** Prec@1 65.750 Prec@5 95.815 Error@1 34.250
  **VAL** Prec@1 84.551 Prec@5 99.157 Error@1 15.449

==>>[2018-03-17 15:00:20] [Epoch=027/300] [Need: 04:13:26] [learning_rate=0.0002] [Best : Accuracy=85.11, Error=14.89]

==>>Epoch=[027/300]], [2018-03-17 15:00:20], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [027][000/127]   Time 0.330 (0.330)   Data 0.293 (0.293)   Loss 0.9546 (0.9546)   Prec@1 75.000 (75.000)   Prec@5 93.750 (93.750)   [2018-03-17 15:00:21]
  **Train** Prec@1 66.667 Prec@5 96.558 Error@1 33.333
  **VAL** Prec@1 83.848 Prec@5 99.017 Error@1 16.152

==>>[2018-03-17 15:01:16] [Epoch=028/300] [Need: 04:12:30] [learning_rate=0.0002] [Best : Accuracy=85.11, Error=14.89]

==>>Epoch=[028/300]], [2018-03-17 15:01:16], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [028][000/127]   Time 0.437 (0.437)   Data 0.400 (0.400)   Loss 1.1788 (1.1788)   Prec@1 68.750 (68.750)   Prec@5 90.625 (90.625)   [2018-03-17 15:01:16]
  **Train** Prec@1 67.112 Prec@5 96.013 Error@1 32.888
  **VAL** Prec@1 74.017 Prec@5 98.034 Error@1 25.983

==>>[2018-03-17 15:02:12] [Epoch=029/300] [Need: 04:11:34] [learning_rate=0.0002] [Best : Accuracy=85.11, Error=14.89]

==>>Epoch=[029/300]], [2018-03-17 15:02:12], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [029][000/127]   Time 0.423 (0.423)   Data 0.387 (0.387)   Loss 1.1220 (1.1220)   Prec@1 59.375 (59.375)   Prec@5 96.875 (96.875)   [2018-03-17 15:02:12]
  **Train** Prec@1 66.196 Prec@5 96.682 Error@1 33.804
  **VAL** Prec@1 83.287 Prec@5 99.298 Error@1 16.713

==>>[2018-03-17 15:03:07] [Epoch=030/300] [Need: 04:10:38] [learning_rate=0.0002] [Best : Accuracy=85.11, Error=14.89]

==>>Epoch=[030/300]], [2018-03-17 15:03:07], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [030][000/127]   Time 0.373 (0.373)   Data 0.338 (0.338)   Loss 1.0514 (1.0514)   Prec@1 65.625 (65.625)   Prec@5 96.875 (96.875)   [2018-03-17 15:03:08]
  **Train** Prec@1 67.211 Prec@5 96.459 Error@1 32.789
  **VAL** Prec@1 83.989 Prec@5 99.438 Error@1 16.011

==>>[2018-03-17 15:04:03] [Epoch=031/300] [Need: 04:09:41] [learning_rate=0.0002] [Best : Accuracy=85.11, Error=14.89]

==>>Epoch=[031/300]], [2018-03-17 15:04:03], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [031][000/127]   Time 0.408 (0.408)   Data 0.373 (0.373)   Loss 1.0057 (1.0057)   Prec@1 65.625 (65.625)   Prec@5 100.000 (100.000)   [2018-03-17 15:04:03]
  **Train** Prec@1 68.029 Prec@5 96.657 Error@1 31.971
  **VAL** Prec@1 87.219 Prec@5 99.157 Error@1 12.781

==>>[2018-03-17 15:04:58] [Epoch=032/300] [Need: 04:08:44] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[032/300]], [2018-03-17 15:04:58], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [032][000/127]   Time 0.479 (0.479)   Data 0.444 (0.444)   Loss 0.7700 (0.7700)   Prec@1 75.000 (75.000)   Prec@5 96.875 (96.875)   [2018-03-17 15:04:59]
  **Train** Prec@1 66.320 Prec@5 96.087 Error@1 33.680
  **VAL** Prec@1 83.427 Prec@5 99.719 Error@1 16.573

==>>[2018-03-17 15:05:54] [Epoch=033/300] [Need: 04:07:48] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[033/300]], [2018-03-17 15:05:54], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [033][000/127]   Time 0.261 (0.261)   Data 0.224 (0.224)   Loss 0.8219 (0.8219)   Prec@1 62.500 (62.500)   Prec@5 100.000 (100.000)   [2018-03-17 15:05:54]
  **Train** Prec@1 68.474 Prec@5 96.137 Error@1 31.526
  **VAL** Prec@1 84.129 Prec@5 99.438 Error@1 15.871

==>>[2018-03-17 15:06:50] [Epoch=034/300] [Need: 04:06:52] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[034/300]], [2018-03-17 15:06:50], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [034][000/127]   Time 0.339 (0.339)   Data 0.302 (0.302)   Loss 1.0248 (1.0248)   Prec@1 62.500 (62.500)   Prec@5 96.875 (96.875)   [2018-03-17 15:06:50]
  **Train** Prec@1 68.252 Prec@5 96.904 Error@1 31.748
  **VAL** Prec@1 85.112 Prec@5 99.438 Error@1 14.888

==>>[2018-03-17 15:07:45] [Epoch=035/300] [Need: 04:05:57] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[035/300]], [2018-03-17 15:07:45], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [035][000/127]   Time 0.292 (0.292)   Data 0.256 (0.256)   Loss 1.0459 (1.0459)   Prec@1 62.500 (62.500)   Prec@5 93.750 (93.750)   [2018-03-17 15:07:46]
  **Train** Prec@1 67.509 Prec@5 96.161 Error@1 32.491
  **VAL** Prec@1 83.567 Prec@5 99.438 Error@1 16.433

==>>[2018-03-17 15:08:41] [Epoch=036/300] [Need: 04:05:00] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[036/300]], [2018-03-17 15:08:41], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [036][000/127]   Time 0.368 (0.368)   Data 0.332 (0.332)   Loss 1.0178 (1.0178)   Prec@1 62.500 (62.500)   Prec@5 100.000 (100.000)   [2018-03-17 15:08:41]
  **Train** Prec@1 68.053 Prec@5 96.261 Error@1 31.947
  **VAL** Prec@1 86.517 Prec@5 99.298 Error@1 13.483

==>>[2018-03-17 15:09:37] [Epoch=037/300] [Need: 04:04:04] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[037/300]], [2018-03-17 15:09:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [037][000/127]   Time 0.301 (0.301)   Data 0.264 (0.264)   Loss 1.1902 (1.1902)   Prec@1 62.500 (62.500)   Prec@5 93.750 (93.750)   [2018-03-17 15:09:37]
  **Train** Prec@1 68.351 Prec@5 96.855 Error@1 31.649
  **VAL** Prec@1 85.955 Prec@5 99.579 Error@1 14.045

==>>[2018-03-17 15:10:32] [Epoch=038/300] [Need: 04:03:07] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[038/300]], [2018-03-17 15:10:32], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [038][000/127]   Time 0.248 (0.248)   Data 0.208 (0.208)   Loss 1.0132 (1.0132)   Prec@1 65.625 (65.625)   Prec@5 90.625 (90.625)   [2018-03-17 15:10:32]
  **Train** Prec@1 69.762 Prec@5 96.954 Error@1 30.238
  **VAL** Prec@1 84.691 Prec@5 99.719 Error@1 15.309

==>>[2018-03-17 15:11:28] [Epoch=039/300] [Need: 04:02:11] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[039/300]], [2018-03-17 15:11:28], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [039][000/127]   Time 0.454 (0.454)   Data 0.417 (0.417)   Loss 0.8709 (0.8709)   Prec@1 71.875 (71.875)   Prec@5 96.875 (96.875)   [2018-03-17 15:11:28]
  **Train** Prec@1 69.242 Prec@5 96.459 Error@1 30.758
  **VAL** Prec@1 85.253 Prec@5 99.298 Error@1 14.747

==>>[2018-03-17 15:12:23] [Epoch=040/300] [Need: 04:01:15] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[040/300]], [2018-03-17 15:12:23], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [040][000/127]   Time 0.430 (0.430)   Data 0.393 (0.393)   Loss 0.7472 (0.7472)   Prec@1 78.125 (78.125)   Prec@5 93.750 (93.750)   [2018-03-17 15:12:24]
  **Train** Prec@1 70.158 Prec@5 96.434 Error@1 29.842
  **VAL** Prec@1 80.618 Prec@5 98.876 Error@1 19.382

==>>[2018-03-17 15:13:19] [Epoch=041/300] [Need: 04:00:19] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[041/300]], [2018-03-17 15:13:19], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [041][000/127]   Time 0.322 (0.322)   Data 0.284 (0.284)   Loss 0.8401 (0.8401)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 15:13:19]
  **Train** Prec@1 69.242 Prec@5 96.384 Error@1 30.758
  **VAL** Prec@1 84.831 Prec@5 99.719 Error@1 15.169

==>>[2018-03-17 15:14:15] [Epoch=042/300] [Need: 03:59:24] [learning_rate=0.0002] [Best : Accuracy=87.22, Error=12.78]

==>>Epoch=[042/300]], [2018-03-17 15:14:15], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [042][000/127]   Time 0.411 (0.411)   Data 0.376 (0.376)   Loss 1.1513 (1.1513)   Prec@1 65.625 (65.625)   Prec@5 87.500 (87.500)   [2018-03-17 15:14:15]
  **Train** Prec@1 70.604 Prec@5 97.003 Error@1 29.396
  **VAL** Prec@1 88.202 Prec@5 99.719 Error@1 11.798

==>>[2018-03-17 15:15:10] [Epoch=043/300] [Need: 03:58:28] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[043/300]], [2018-03-17 15:15:10], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [043][000/127]   Time 0.410 (0.410)   Data 0.375 (0.375)   Loss 0.4801 (0.4801)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 15:15:11]
  **Train** Prec@1 70.059 Prec@5 96.508 Error@1 29.941
  **VAL** Prec@1 87.640 Prec@5 99.438 Error@1 12.360

==>>[2018-03-17 15:16:06] [Epoch=044/300] [Need: 03:57:31] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[044/300]], [2018-03-17 15:16:06], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [044][000/127]   Time 0.358 (0.358)   Data 0.322 (0.322)   Loss 0.3684 (0.3684)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 15:16:06]
  **Train** Prec@1 69.787 Prec@5 96.954 Error@1 30.213
  **VAL** Prec@1 86.798 Prec@5 99.157 Error@1 13.202

==>>[2018-03-17 15:17:01] [Epoch=045/300] [Need: 03:56:35] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[045/300]], [2018-03-17 15:17:01], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [045][000/127]   Time 0.260 (0.260)   Data 0.224 (0.224)   Loss 0.9448 (0.9448)   Prec@1 68.750 (68.750)   Prec@5 96.875 (96.875)   [2018-03-17 15:17:02]
  **Train** Prec@1 70.976 Prec@5 97.028 Error@1 29.024
  **VAL** Prec@1 85.674 Prec@5 99.719 Error@1 14.326

==>>[2018-03-17 15:17:57] [Epoch=046/300] [Need: 03:55:39] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[046/300]], [2018-03-17 15:17:57], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [046][000/127]   Time 0.359 (0.359)   Data 0.324 (0.324)   Loss 1.1967 (1.1967)   Prec@1 59.375 (59.375)   Prec@5 96.875 (96.875)   [2018-03-17 15:17:57]
  **Train** Prec@1 70.728 Prec@5 97.202 Error@1 29.272
  **VAL** Prec@1 85.112 Prec@5 99.579 Error@1 14.888

==>>[2018-03-17 15:18:53] [Epoch=047/300] [Need: 03:54:43] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[047/300]], [2018-03-17 15:18:53], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [047][000/127]   Time 0.354 (0.354)   Data 0.318 (0.318)   Loss 0.7957 (0.7957)   Prec@1 71.875 (71.875)   Prec@5 100.000 (100.000)   [2018-03-17 15:18:53]
  **Train** Prec@1 71.942 Prec@5 96.781 Error@1 28.058
  **VAL** Prec@1 85.253 Prec@5 99.860 Error@1 14.747

==>>[2018-03-17 15:19:48] [Epoch=048/300] [Need: 03:53:47] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[048/300]], [2018-03-17 15:19:48], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [048][000/127]   Time 0.316 (0.316)   Data 0.280 (0.280)   Loss 1.1884 (1.1884)   Prec@1 62.500 (62.500)   Prec@5 93.750 (93.750)   [2018-03-17 15:19:48]
  **Train** Prec@1 70.357 Prec@5 97.127 Error@1 29.643
  **VAL** Prec@1 87.219 Prec@5 99.579 Error@1 12.781

==>>[2018-03-17 15:20:44] [Epoch=049/300] [Need: 03:52:50] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[049/300]], [2018-03-17 15:20:44], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [049][000/127]   Time 0.382 (0.382)   Data 0.347 (0.347)   Loss 0.5088 (0.5088)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 15:20:44]
  **Train** Prec@1 72.214 Prec@5 97.276 Error@1 27.786
  **VAL** Prec@1 86.517 Prec@5 99.579 Error@1 13.483

==>>[2018-03-17 15:21:39] [Epoch=050/300] [Need: 03:51:54] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[050/300]], [2018-03-17 15:21:39], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [050][000/127]   Time 0.500 (0.500)   Data 0.465 (0.465)   Loss 1.2684 (1.2684)   Prec@1 62.500 (62.500)   Prec@5 93.750 (93.750)   [2018-03-17 15:21:40]
  **Train** Prec@1 71.991 Prec@5 97.078 Error@1 28.009
  **VAL** Prec@1 83.567 Prec@5 99.298 Error@1 16.433

==>>[2018-03-17 15:22:35] [Epoch=051/300] [Need: 03:50:58] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[051/300]], [2018-03-17 15:22:35], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [051][000/127]   Time 0.266 (0.266)   Data 0.229 (0.229)   Loss 0.8553 (0.8553)   Prec@1 68.750 (68.750)   Prec@5 100.000 (100.000)   [2018-03-17 15:22:35]
  **Train** Prec@1 72.090 Prec@5 96.731 Error@1 27.910
  **VAL** Prec@1 86.517 Prec@5 99.579 Error@1 13.483

==>>[2018-03-17 15:23:30] [Epoch=052/300] [Need: 03:50:02] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[052/300]], [2018-03-17 15:23:30], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [052][000/127]   Time 0.481 (0.481)   Data 0.445 (0.445)   Loss 0.5210 (0.5210)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 15:23:31]
  **Train** Prec@1 73.774 Prec@5 97.573 Error@1 26.226
  **VAL** Prec@1 84.691 Prec@5 99.579 Error@1 15.309

==>>[2018-03-17 15:24:26] [Epoch=053/300] [Need: 03:49:06] [learning_rate=0.0002] [Best : Accuracy=88.20, Error=11.80]

==>>Epoch=[053/300]], [2018-03-17 15:24:26], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [053][000/127]   Time 0.307 (0.307)   Data 0.271 (0.271)   Loss 0.8279 (0.8279)   Prec@1 75.000 (75.000)   Prec@5 100.000 (100.000)   [2018-03-17 15:24:26]
  **Train** Prec@1 74.146 Prec@5 97.474 Error@1 25.854
  **VAL** Prec@1 89.045 Prec@5 99.860 Error@1 10.955

==>>[2018-03-17 15:25:21] [Epoch=054/300] [Need: 03:48:10] [learning_rate=0.0002] [Best : Accuracy=89.04, Error=10.96]

==>>Epoch=[054/300]], [2018-03-17 15:25:21], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [054][000/127]   Time 0.345 (0.345)   Data 0.310 (0.310)   Loss 0.6132 (0.6132)   Prec@1 84.375 (84.375)   Prec@5 96.875 (96.875)   [2018-03-17 15:25:22]
  **Train** Prec@1 73.749 Prec@5 97.053 Error@1 26.251
  **VAL** Prec@1 84.831 Prec@5 98.876 Error@1 15.169

==>>[2018-03-17 15:26:17] [Epoch=055/300] [Need: 03:47:14] [learning_rate=0.0002] [Best : Accuracy=89.04, Error=10.96]

==>>Epoch=[055/300]], [2018-03-17 15:26:17], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [055][000/127]   Time 0.265 (0.265)   Data 0.230 (0.230)   Loss 0.7079 (0.7079)   Prec@1 65.625 (65.625)   Prec@5 100.000 (100.000)   [2018-03-17 15:26:17]
  **Train** Prec@1 73.304 Prec@5 97.226 Error@1 26.696
  **VAL** Prec@1 87.360 Prec@5 99.438 Error@1 12.640

==>>[2018-03-17 15:27:13] [Epoch=056/300] [Need: 03:46:17] [learning_rate=0.0002] [Best : Accuracy=89.04, Error=10.96]

==>>Epoch=[056/300]], [2018-03-17 15:27:13], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [056][000/127]   Time 0.311 (0.311)   Data 0.276 (0.276)   Loss 0.9806 (0.9806)   Prec@1 68.750 (68.750)   Prec@5 90.625 (90.625)   [2018-03-17 15:27:13]
  **Train** Prec@1 73.328 Prec@5 97.375 Error@1 26.672
  **VAL** Prec@1 75.702 Prec@5 98.876 Error@1 24.298

==>>[2018-03-17 15:28:08] [Epoch=057/300] [Need: 03:45:22] [learning_rate=0.0002] [Best : Accuracy=89.04, Error=10.96]

==>>Epoch=[057/300]], [2018-03-17 15:28:08], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [057][000/127]   Time 0.307 (0.307)   Data 0.272 (0.272)   Loss 0.8434 (0.8434)   Prec@1 68.750 (68.750)   Prec@5 96.875 (96.875)   [2018-03-17 15:28:08]
  **Train** Prec@1 73.526 Prec@5 97.524 Error@1 26.474
  **VAL** Prec@1 89.045 Prec@5 99.719 Error@1 10.955

==>>[2018-03-17 15:29:04] [Epoch=058/300] [Need: 03:44:25] [learning_rate=0.0002] [Best : Accuracy=89.04, Error=10.96]

==>>Epoch=[058/300]], [2018-03-17 15:29:04], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [058][000/127]   Time 0.578 (0.578)   Data 0.542 (0.542)   Loss 0.8541 (0.8541)   Prec@1 71.875 (71.875)   Prec@5 93.750 (93.750)   [2018-03-17 15:29:04]
  **Train** Prec@1 72.511 Prec@5 97.623 Error@1 27.489
  **VAL** Prec@1 84.129 Prec@5 99.438 Error@1 15.871

==>>[2018-03-17 15:29:59] [Epoch=059/300] [Need: 03:43:30] [learning_rate=0.0002] [Best : Accuracy=89.04, Error=10.96]

==>>Epoch=[059/300]], [2018-03-17 15:29:59], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [059][000/127]   Time 0.447 (0.447)   Data 0.412 (0.412)   Loss 0.8157 (0.8157)   Prec@1 68.750 (68.750)   Prec@5 100.000 (100.000)   [2018-03-17 15:30:00]
  **Train** Prec@1 73.526 Prec@5 97.375 Error@1 26.474
  **VAL** Prec@1 89.045 Prec@5 99.719 Error@1 10.955

==>>[2018-03-17 15:30:55] [Epoch=060/300] [Need: 03:42:33] [learning_rate=0.0002] [Best : Accuracy=89.04, Error=10.96]

==>>Epoch=[060/300]], [2018-03-17 15:30:55], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [060][000/127]   Time 0.274 (0.274)   Data 0.238 (0.238)   Loss 0.4652 (0.4652)   Prec@1 93.750 (93.750)   Prec@5 96.875 (96.875)   [2018-03-17 15:30:55]
  **Train** Prec@1 75.557 Prec@5 97.598 Error@1 24.443
  **VAL** Prec@1 89.466 Prec@5 99.860 Error@1 10.534

==>>[2018-03-17 15:31:50] [Epoch=061/300] [Need: 03:41:37] [learning_rate=0.0002] [Best : Accuracy=89.47, Error=10.53]

==>>Epoch=[061/300]], [2018-03-17 15:31:50], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [061][000/127]   Time 0.426 (0.426)   Data 0.390 (0.390)   Loss 0.7669 (0.7669)   Prec@1 68.750 (68.750)   Prec@5 93.750 (93.750)   [2018-03-17 15:31:51]
  **Train** Prec@1 74.071 Prec@5 97.226 Error@1 25.929
  **VAL** Prec@1 90.871 Prec@5 99.860 Error@1 9.129

==>>[2018-03-17 15:32:46] [Epoch=062/300] [Need: 03:40:41] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[062/300]], [2018-03-17 15:32:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [062][000/127]   Time 0.541 (0.541)   Data 0.506 (0.506)   Loss 0.9647 (0.9647)   Prec@1 65.625 (65.625)   Prec@5 93.750 (93.750)   [2018-03-17 15:32:46]
  **Train** Prec@1 74.170 Prec@5 97.152 Error@1 25.830
  **VAL** Prec@1 77.809 Prec@5 98.876 Error@1 22.191

==>>[2018-03-17 15:33:41] [Epoch=063/300] [Need: 03:39:45] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[063/300]], [2018-03-17 15:33:41], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [063][000/127]   Time 0.394 (0.394)   Data 0.358 (0.358)   Loss 1.0905 (1.0905)   Prec@1 71.875 (71.875)   Prec@5 93.750 (93.750)   [2018-03-17 15:33:42]
  **Train** Prec@1 74.492 Prec@5 97.400 Error@1 25.508
  **VAL** Prec@1 89.466 Prec@5 99.719 Error@1 10.534

==>>[2018-03-17 15:34:37] [Epoch=064/300] [Need: 03:38:49] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[064/300]], [2018-03-17 15:34:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [064][000/127]   Time 0.362 (0.362)   Data 0.327 (0.327)   Loss 0.3417 (0.3417)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 15:34:37]
  **Train** Prec@1 74.814 Prec@5 97.474 Error@1 25.186
  **VAL** Prec@1 88.624 Prec@5 99.719 Error@1 11.376

==>>[2018-03-17 15:35:32] [Epoch=065/300] [Need: 03:37:53] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[065/300]], [2018-03-17 15:35:32], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [065][000/127]   Time 0.430 (0.430)   Data 0.394 (0.394)   Loss 0.9287 (0.9287)   Prec@1 59.375 (59.375)   Prec@5 100.000 (100.000)   [2018-03-17 15:35:33]
  **Train** Prec@1 73.626 Prec@5 97.598 Error@1 26.374
  **VAL** Prec@1 83.567 Prec@5 99.157 Error@1 16.433

==>>[2018-03-17 15:36:28] [Epoch=066/300] [Need: 03:36:59] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[066/300]], [2018-03-17 15:36:28], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [066][000/127]   Time 0.474 (0.474)   Data 0.437 (0.437)   Loss 0.6735 (0.6735)   Prec@1 71.875 (71.875)   Prec@5 96.875 (96.875)   [2018-03-17 15:36:29]
  **Train** Prec@1 75.334 Prec@5 97.647 Error@1 24.666
  **VAL** Prec@1 89.888 Prec@5 99.719 Error@1 10.112

==>>[2018-03-17 15:37:24] [Epoch=067/300] [Need: 03:36:04] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[067/300]], [2018-03-17 15:37:24], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [067][000/127]   Time 0.331 (0.331)   Data 0.296 (0.296)   Loss 0.8011 (0.8011)   Prec@1 68.750 (68.750)   Prec@5 100.000 (100.000)   [2018-03-17 15:37:25]
  **Train** Prec@1 74.368 Prec@5 97.598 Error@1 25.632
  **VAL** Prec@1 86.376 Prec@5 99.438 Error@1 13.624

==>>[2018-03-17 15:38:20] [Epoch=068/300] [Need: 03:35:09] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[068/300]], [2018-03-17 15:38:20], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [068][000/127]   Time 0.351 (0.351)   Data 0.314 (0.314)   Loss 0.7567 (0.7567)   Prec@1 71.875 (71.875)   Prec@5 100.000 (100.000)   [2018-03-17 15:38:21]
  **Train** Prec@1 76.350 Prec@5 98.019 Error@1 23.650
  **VAL** Prec@1 89.747 Prec@5 99.719 Error@1 10.253

==>>[2018-03-17 15:39:16] [Epoch=069/300] [Need: 03:34:14] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[069/300]], [2018-03-17 15:39:16], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [069][000/127]   Time 0.406 (0.406)   Data 0.369 (0.369)   Loss 0.7936 (0.7936)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 15:39:16]
  **Train** Prec@1 76.251 Prec@5 97.548 Error@1 23.749
  **VAL** Prec@1 88.904 Prec@5 99.719 Error@1 11.096

==>>[2018-03-17 15:40:12] [Epoch=070/300] [Need: 03:33:18] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[070/300]], [2018-03-17 15:40:12], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [070][000/127]   Time 0.325 (0.325)   Data 0.289 (0.289)   Loss 0.7170 (0.7170)   Prec@1 62.500 (62.500)   Prec@5 100.000 (100.000)   [2018-03-17 15:40:12]
  **Train** Prec@1 73.947 Prec@5 97.276 Error@1 26.053
  **VAL** Prec@1 90.871 Prec@5 99.719 Error@1 9.129

==>>[2018-03-17 15:41:07] [Epoch=071/300] [Need: 03:32:23] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[071/300]], [2018-03-17 15:41:07], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [071][000/127]   Time 0.423 (0.423)   Data 0.385 (0.385)   Loss 0.7232 (0.7232)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 15:41:08]
  **Train** Prec@1 76.498 Prec@5 97.647 Error@1 23.502
  **VAL** Prec@1 89.185 Prec@5 99.719 Error@1 10.815

==>>[2018-03-17 15:42:03] [Epoch=072/300] [Need: 03:31:28] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[072/300]], [2018-03-17 15:42:03], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [072][000/127]   Time 0.463 (0.463)   Data 0.428 (0.428)   Loss 0.5269 (0.5269)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 15:42:03]
  **Train** Prec@1 76.523 Prec@5 97.746 Error@1 23.477
  **VAL** Prec@1 85.815 Prec@5 99.860 Error@1 14.185

==>>[2018-03-17 15:42:59] [Epoch=073/300] [Need: 03:30:32] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[073/300]], [2018-03-17 15:42:59], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [073][000/127]   Time 0.402 (0.402)   Data 0.365 (0.365)   Loss 0.8435 (0.8435)   Prec@1 62.500 (62.500)   Prec@5 100.000 (100.000)   [2018-03-17 15:42:59]
  **Train** Prec@1 75.755 Prec@5 97.598 Error@1 24.245
  **VAL** Prec@1 90.449 Prec@5 99.719 Error@1 9.551

==>>[2018-03-17 15:43:54] [Epoch=074/300] [Need: 03:29:37] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[074/300]], [2018-03-17 15:43:54], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [074][000/127]   Time 0.332 (0.332)   Data 0.295 (0.295)   Loss 0.5421 (0.5421)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 15:43:55]
  **Train** Prec@1 75.755 Prec@5 97.796 Error@1 24.245
  **VAL** Prec@1 88.202 Prec@5 99.719 Error@1 11.798

==>>[2018-03-17 15:44:50] [Epoch=075/300] [Need: 03:28:41] [learning_rate=0.0002] [Best : Accuracy=90.87, Error=9.13]

==>>Epoch=[075/300]], [2018-03-17 15:44:50], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [075][000/127]   Time 0.447 (0.447)   Data 0.411 (0.411)   Loss 0.7689 (0.7689)   Prec@1 71.875 (71.875)   Prec@5 96.875 (96.875)   [2018-03-17 15:44:51]
  **Train** Prec@1 76.053 Prec@5 97.647 Error@1 23.947
  **VAL** Prec@1 91.713 Prec@5 99.860 Error@1 8.287

==>>[2018-03-17 15:45:46] [Epoch=076/300] [Need: 03:27:46] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[076/300]], [2018-03-17 15:45:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [076][000/127]   Time 0.427 (0.427)   Data 0.389 (0.389)   Loss 0.9776 (0.9776)   Prec@1 65.625 (65.625)   Prec@5 90.625 (90.625)   [2018-03-17 15:45:46]
  **Train** Prec@1 76.919 Prec@5 97.969 Error@1 23.081
  **VAL** Prec@1 91.292 Prec@5 99.719 Error@1 8.708

==>>[2018-03-17 15:46:42] [Epoch=077/300] [Need: 03:26:50] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[077/300]], [2018-03-17 15:46:42], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [077][000/127]   Time 0.341 (0.341)   Data 0.303 (0.303)   Loss 0.6460 (0.6460)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 15:46:42]
  **Train** Prec@1 75.805 Prec@5 97.573 Error@1 24.195
  **VAL** Prec@1 90.449 Prec@5 99.719 Error@1 9.551

==>>[2018-03-17 15:47:37] [Epoch=078/300] [Need: 03:25:55] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[078/300]], [2018-03-17 15:47:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [078][000/127]   Time 0.398 (0.398)   Data 0.360 (0.360)   Loss 0.5691 (0.5691)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 15:47:38]
  **Train** Prec@1 76.895 Prec@5 97.821 Error@1 23.105
  **VAL** Prec@1 88.764 Prec@5 99.719 Error@1 11.236

==>>[2018-03-17 15:48:33] [Epoch=079/300] [Need: 03:24:59] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[079/300]], [2018-03-17 15:48:33], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [079][000/127]   Time 0.328 (0.328)   Data 0.293 (0.293)   Loss 0.8235 (0.8235)   Prec@1 75.000 (75.000)   Prec@5 93.750 (93.750)   [2018-03-17 15:48:33]
  **Train** Prec@1 77.885 Prec@5 98.167 Error@1 22.115
  **VAL** Prec@1 83.287 Prec@5 99.298 Error@1 16.713

==>>[2018-03-17 15:49:29] [Epoch=080/300] [Need: 03:24:04] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[080/300]], [2018-03-17 15:49:29], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [080][000/127]   Time 0.280 (0.280)   Data 0.244 (0.244)   Loss 0.7040 (0.7040)   Prec@1 81.250 (81.250)   Prec@5 96.875 (96.875)   [2018-03-17 15:49:29]
  **Train** Prec@1 76.251 Prec@5 98.068 Error@1 23.749
  **VAL** Prec@1 87.219 Prec@5 99.298 Error@1 12.781

==>>[2018-03-17 15:50:25] [Epoch=081/300] [Need: 03:23:08] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[081/300]], [2018-03-17 15:50:25], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [081][000/127]   Time 0.514 (0.514)   Data 0.477 (0.477)   Loss 1.0094 (1.0094)   Prec@1 65.625 (65.625)   Prec@5 90.625 (90.625)   [2018-03-17 15:50:25]
  **Train** Prec@1 76.176 Prec@5 97.796 Error@1 23.824
  **VAL** Prec@1 91.011 Prec@5 99.719 Error@1 8.989

==>>[2018-03-17 15:51:20] [Epoch=082/300] [Need: 03:22:13] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[082/300]], [2018-03-17 15:51:20], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [082][000/127]   Time 0.312 (0.312)   Data 0.275 (0.275)   Loss 0.5115 (0.5115)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 15:51:21]
  **Train** Prec@1 77.241 Prec@5 98.167 Error@1 22.759
  **VAL** Prec@1 90.309 Prec@5 99.579 Error@1 9.691

==>>[2018-03-17 15:52:16] [Epoch=083/300] [Need: 03:21:17] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[083/300]], [2018-03-17 15:52:16], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [083][000/127]   Time 0.353 (0.353)   Data 0.318 (0.318)   Loss 0.3609 (0.3609)   Prec@1 90.625 (90.625)   Prec@5 96.875 (96.875)   [2018-03-17 15:52:16]
  **Train** Prec@1 76.275 Prec@5 97.895 Error@1 23.725
  **VAL** Prec@1 88.764 Prec@5 99.860 Error@1 11.236

==>>[2018-03-17 15:53:12] [Epoch=084/300] [Need: 03:20:22] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[084/300]], [2018-03-17 15:53:12], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [084][000/127]   Time 0.323 (0.323)   Data 0.286 (0.286)   Loss 0.6441 (0.6441)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 15:53:12]
  **Train** Prec@1 77.712 Prec@5 97.969 Error@1 22.288
  **VAL** Prec@1 88.202 Prec@5 99.579 Error@1 11.798

==>>[2018-03-17 15:54:08] [Epoch=085/300] [Need: 03:19:27] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[085/300]], [2018-03-17 15:54:08], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [085][000/127]   Time 0.330 (0.330)   Data 0.293 (0.293)   Loss 0.4291 (0.4291)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 15:54:08]
  **Train** Prec@1 76.944 Prec@5 97.771 Error@1 23.056
  **VAL** Prec@1 88.483 Prec@5 99.860 Error@1 11.517

==>>[2018-03-17 15:55:03] [Epoch=086/300] [Need: 03:18:31] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[086/300]], [2018-03-17 15:55:03], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [086][000/127]   Time 0.363 (0.363)   Data 0.328 (0.328)   Loss 0.2857 (0.2857)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-03-17 15:55:04]
  **Train** Prec@1 76.994 Prec@5 97.821 Error@1 23.006
  **VAL** Prec@1 86.517 Prec@5 99.860 Error@1 13.483

==>>[2018-03-17 15:55:59] [Epoch=087/300] [Need: 03:17:35] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[087/300]], [2018-03-17 15:55:59], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [087][000/127]   Time 0.377 (0.377)   Data 0.340 (0.340)   Loss 0.8366 (0.8366)   Prec@1 71.875 (71.875)   Prec@5 90.625 (90.625)   [2018-03-17 15:55:59]
  **Train** Prec@1 77.737 Prec@5 97.945 Error@1 22.263
  **VAL** Prec@1 90.449 Prec@5 99.860 Error@1 9.551

==>>[2018-03-17 15:56:55] [Epoch=088/300] [Need: 03:16:40] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[088/300]], [2018-03-17 15:56:55], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [088][000/127]   Time 0.330 (0.330)   Data 0.293 (0.293)   Loss 0.9412 (0.9412)   Prec@1 71.875 (71.875)   Prec@5 90.625 (90.625)   [2018-03-17 15:56:55]
  **Train** Prec@1 76.746 Prec@5 97.647 Error@1 23.254
  **VAL** Prec@1 87.500 Prec@5 100.000 Error@1 12.500

==>>[2018-03-17 15:57:50] [Epoch=089/300] [Need: 03:15:44] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[089/300]], [2018-03-17 15:57:50], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [089][000/127]   Time 0.513 (0.513)   Data 0.476 (0.476)   Loss 0.7848 (0.7848)   Prec@1 68.750 (68.750)   Prec@5 96.875 (96.875)   [2018-03-17 15:57:51]
  **Train** Prec@1 79.099 Prec@5 98.217 Error@1 20.901
  **VAL** Prec@1 89.045 Prec@5 100.000 Error@1 10.955

==>>[2018-03-17 15:58:46] [Epoch=090/300] [Need: 03:14:49] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[090/300]], [2018-03-17 15:58:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [090][000/127]   Time 0.484 (0.484)   Data 0.449 (0.449)   Loss 0.7022 (0.7022)   Prec@1 75.000 (75.000)   Prec@5 96.875 (96.875)   [2018-03-17 15:58:46]
  **Train** Prec@1 78.034 Prec@5 98.044 Error@1 21.966
  **VAL** Prec@1 90.449 Prec@5 99.719 Error@1 9.551

==>>[2018-03-17 15:59:42] [Epoch=091/300] [Need: 03:13:53] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[091/300]], [2018-03-17 15:59:42], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [091][000/127]   Time 0.385 (0.385)   Data 0.349 (0.349)   Loss 0.6345 (0.6345)   Prec@1 75.000 (75.000)   Prec@5 100.000 (100.000)   [2018-03-17 15:59:42]
  **Train** Prec@1 78.257 Prec@5 97.449 Error@1 21.743
  **VAL** Prec@1 89.185 Prec@5 99.860 Error@1 10.815

==>>[2018-03-17 16:00:37] [Epoch=092/300] [Need: 03:12:57] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[092/300]], [2018-03-17 16:00:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [092][000/127]   Time 0.472 (0.472)   Data 0.435 (0.435)   Loss 0.4451 (0.4451)   Prec@1 84.375 (84.375)   Prec@5 96.875 (96.875)   [2018-03-17 16:00:38]
  **Train** Prec@1 77.935 Prec@5 97.969 Error@1 22.065
  **VAL** Prec@1 89.747 Prec@5 99.860 Error@1 10.253

==>>[2018-03-17 16:01:33] [Epoch=093/300] [Need: 03:12:02] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[093/300]], [2018-03-17 16:01:33], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [093][000/127]   Time 0.347 (0.347)   Data 0.307 (0.307)   Loss 0.9802 (0.9802)   Prec@1 71.875 (71.875)   Prec@5 93.750 (93.750)   [2018-03-17 16:01:33]
  **Train** Prec@1 77.910 Prec@5 97.969 Error@1 22.090
  **VAL** Prec@1 89.607 Prec@5 100.000 Error@1 10.393

==>>[2018-03-17 16:02:29] [Epoch=094/300] [Need: 03:11:06] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[094/300]], [2018-03-17 16:02:29], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [094][000/127]   Time 0.352 (0.352)   Data 0.315 (0.315)   Loss 0.4674 (0.4674)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:02:29]
  **Train** Prec@1 77.514 Prec@5 98.366 Error@1 22.486
  **VAL** Prec@1 90.871 Prec@5 99.579 Error@1 9.129

==>>[2018-03-17 16:03:24] [Epoch=095/300] [Need: 03:10:11] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[095/300]], [2018-03-17 16:03:24], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [095][000/127]   Time 0.715 (0.715)   Data 0.678 (0.678)   Loss 0.5509 (0.5509)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 16:03:25]
  **Train** Prec@1 78.380 Prec@5 98.167 Error@1 21.620
  **VAL** Prec@1 89.045 Prec@5 99.860 Error@1 10.955

==>>[2018-03-17 16:04:20] [Epoch=096/300] [Need: 03:09:15] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[096/300]], [2018-03-17 16:04:20], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [096][000/127]   Time 0.321 (0.321)   Data 0.286 (0.286)   Loss 0.6679 (0.6679)   Prec@1 78.125 (78.125)   Prec@5 90.625 (90.625)   [2018-03-17 16:04:21]
  **Train** Prec@1 77.860 Prec@5 97.722 Error@1 22.140
  **VAL** Prec@1 88.764 Prec@5 99.579 Error@1 11.236

==>>[2018-03-17 16:05:16] [Epoch=097/300] [Need: 03:08:20] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[097/300]], [2018-03-17 16:05:16], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [097][000/127]   Time 0.357 (0.357)   Data 0.321 (0.321)   Loss 1.0280 (1.0280)   Prec@1 65.625 (65.625)   Prec@5 96.875 (96.875)   [2018-03-17 16:05:16]
  **Train** Prec@1 78.455 Prec@5 98.242 Error@1 21.545
  **VAL** Prec@1 90.590 Prec@5 100.000 Error@1 9.410

==>>[2018-03-17 16:06:12] [Epoch=098/300] [Need: 03:07:24] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[098/300]], [2018-03-17 16:06:12], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [098][000/127]   Time 0.302 (0.302)   Data 0.267 (0.267)   Loss 0.7185 (0.7185)   Prec@1 81.250 (81.250)   Prec@5 96.875 (96.875)   [2018-03-17 16:06:12]
  **Train** Prec@1 78.479 Prec@5 98.118 Error@1 21.521
  **VAL** Prec@1 86.657 Prec@5 99.860 Error@1 13.343

==>>[2018-03-17 16:07:07] [Epoch=099/300] [Need: 03:06:28] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[099/300]], [2018-03-17 16:07:07], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [099][000/127]   Time 0.284 (0.284)   Data 0.247 (0.247)   Loss 0.8715 (0.8715)   Prec@1 65.625 (65.625)   Prec@5 93.750 (93.750)   [2018-03-17 16:07:07]
  **Train** Prec@1 77.910 Prec@5 98.044 Error@1 22.090
  **VAL** Prec@1 91.433 Prec@5 99.860 Error@1 8.567

==>>[2018-03-17 16:08:03] [Epoch=100/300] [Need: 03:05:33] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[100/300]], [2018-03-17 16:08:03], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [100][000/127]   Time 0.303 (0.303)   Data 0.266 (0.266)   Loss 1.0793 (1.0793)   Prec@1 62.500 (62.500)   Prec@5 96.875 (96.875)   [2018-03-17 16:08:03]
  **Train** Prec@1 78.653 Prec@5 97.920 Error@1 21.347
  **VAL** Prec@1 90.449 Prec@5 99.719 Error@1 9.551

==>>[2018-03-17 16:08:58] [Epoch=101/300] [Need: 03:04:37] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[101/300]], [2018-03-17 16:08:58], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [101][000/127]   Time 0.356 (0.356)   Data 0.320 (0.320)   Loss 0.4887 (0.4887)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 16:08:59]
  **Train** Prec@1 78.727 Prec@5 97.870 Error@1 21.273
  **VAL** Prec@1 90.028 Prec@5 99.860 Error@1 9.972

==>>[2018-03-17 16:09:54] [Epoch=102/300] [Need: 03:03:41] [learning_rate=0.0002] [Best : Accuracy=91.71, Error=8.29]

==>>Epoch=[102/300]], [2018-03-17 16:09:54], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [102][000/127]   Time 0.359 (0.359)   Data 0.322 (0.322)   Loss 0.6032 (0.6032)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 16:09:55]
  **Train** Prec@1 78.158 Prec@5 98.192 Error@1 21.842
  **VAL** Prec@1 92.978 Prec@5 99.860 Error@1 7.022

==>>[2018-03-17 16:10:50] [Epoch=103/300] [Need: 03:02:46] [learning_rate=0.0002] [Best : Accuracy=92.98, Error=7.02]

==>>Epoch=[103/300]], [2018-03-17 16:10:50], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [103][000/127]   Time 0.340 (0.340)   Data 0.303 (0.303)   Loss 0.4858 (0.4858)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:10:50]
  **Train** Prec@1 78.603 Prec@5 98.068 Error@1 21.397
  **VAL** Prec@1 92.135 Prec@5 99.719 Error@1 7.865

==>>[2018-03-17 16:11:46] [Epoch=104/300] [Need: 03:01:50] [learning_rate=0.0002] [Best : Accuracy=92.98, Error=7.02]

==>>Epoch=[104/300]], [2018-03-17 16:11:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [104][000/127]   Time 0.541 (0.541)   Data 0.505 (0.505)   Loss 0.4653 (0.4653)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 16:11:46]
  **Train** Prec@1 78.826 Prec@5 97.920 Error@1 21.174
  **VAL** Prec@1 88.483 Prec@5 99.719 Error@1 11.517

==>>[2018-03-17 16:12:41] [Epoch=105/300] [Need: 03:00:54] [learning_rate=0.0002] [Best : Accuracy=92.98, Error=7.02]

==>>Epoch=[105/300]], [2018-03-17 16:12:41], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [105][000/127]   Time 0.322 (0.322)   Data 0.287 (0.287)   Loss 0.7237 (0.7237)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 16:12:42]
  **Train** Prec@1 79.321 Prec@5 98.242 Error@1 20.679
  **VAL** Prec@1 92.275 Prec@5 99.860 Error@1 7.725

==>>[2018-03-17 16:13:37] [Epoch=106/300] [Need: 02:59:59] [learning_rate=0.0002] [Best : Accuracy=92.98, Error=7.02]

==>>Epoch=[106/300]], [2018-03-17 16:13:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [106][000/127]   Time 0.312 (0.312)   Data 0.277 (0.277)   Loss 0.7621 (0.7621)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 16:13:37]
  **Train** Prec@1 79.148 Prec@5 97.647 Error@1 20.852
  **VAL** Prec@1 92.275 Prec@5 100.000 Error@1 7.725

==>>[2018-03-17 16:14:32] [Epoch=107/300] [Need: 02:59:03] [learning_rate=0.0002] [Best : Accuracy=92.98, Error=7.02]

==>>Epoch=[107/300]], [2018-03-17 16:14:32], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [107][000/127]   Time 0.396 (0.396)   Data 0.360 (0.360)   Loss 0.6914 (0.6914)   Prec@1 68.750 (68.750)   Prec@5 100.000 (100.000)   [2018-03-17 16:14:33]
  **Train** Prec@1 78.257 Prec@5 97.969 Error@1 21.743
  **VAL** Prec@1 89.888 Prec@5 99.579 Error@1 10.112

==>>[2018-03-17 16:15:28] [Epoch=108/300] [Need: 02:58:07] [learning_rate=0.0002] [Best : Accuracy=92.98, Error=7.02]

==>>Epoch=[108/300]], [2018-03-17 16:15:28], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [108][000/127]   Time 0.302 (0.302)   Data 0.267 (0.267)   Loss 0.5518 (0.5518)   Prec@1 87.500 (87.500)   Prec@5 96.875 (96.875)   [2018-03-17 16:15:28]
  **Train** Prec@1 78.925 Prec@5 98.217 Error@1 21.075
  **VAL** Prec@1 84.831 Prec@5 98.876 Error@1 15.169

==>>[2018-03-17 16:16:24] [Epoch=109/300] [Need: 02:57:11] [learning_rate=0.0002] [Best : Accuracy=92.98, Error=7.02]

==>>Epoch=[109/300]], [2018-03-17 16:16:24], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [109][000/127]   Time 0.434 (0.434)   Data 0.398 (0.398)   Loss 0.7368 (0.7368)   Prec@1 84.375 (84.375)   Prec@5 93.750 (93.750)   [2018-03-17 16:16:24]
  **Train** Prec@1 79.049 Prec@5 98.465 Error@1 20.951
  **VAL** Prec@1 93.118 Prec@5 99.719 Error@1 6.882

==>>[2018-03-17 16:17:19] [Epoch=110/300] [Need: 02:56:16] [learning_rate=0.0002] [Best : Accuracy=93.12, Error=6.88]

==>>Epoch=[110/300]], [2018-03-17 16:17:19], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [110][000/127]   Time 0.378 (0.378)   Data 0.342 (0.342)   Loss 0.6905 (0.6905)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 16:17:20]
  **Train** Prec@1 79.024 Prec@5 97.821 Error@1 20.976
  **VAL** Prec@1 91.292 Prec@5 99.860 Error@1 8.708

==>>[2018-03-17 16:18:15] [Epoch=111/300] [Need: 02:55:20] [learning_rate=0.0002] [Best : Accuracy=93.12, Error=6.88]

==>>Epoch=[111/300]], [2018-03-17 16:18:15], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [111][000/127]   Time 0.320 (0.320)   Data 0.284 (0.284)   Loss 0.8296 (0.8296)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 16:18:15]
  **Train** Prec@1 80.584 Prec@5 98.687 Error@1 19.416
  **VAL** Prec@1 89.185 Prec@5 99.860 Error@1 10.815

==>>[2018-03-17 16:19:10] [Epoch=112/300] [Need: 02:54:24] [learning_rate=0.0002] [Best : Accuracy=93.12, Error=6.88]

==>>Epoch=[112/300]], [2018-03-17 16:19:10], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [112][000/127]   Time 0.514 (0.514)   Data 0.478 (0.478)   Loss 0.5974 (0.5974)   Prec@1 84.375 (84.375)   Prec@5 93.750 (93.750)   [2018-03-17 16:19:11]
  **Train** Prec@1 79.594 Prec@5 98.143 Error@1 20.406
  **VAL** Prec@1 85.955 Prec@5 100.000 Error@1 14.045

==>>[2018-03-17 16:20:06] [Epoch=113/300] [Need: 02:53:28] [learning_rate=0.0002] [Best : Accuracy=93.12, Error=6.88]

==>>Epoch=[113/300]], [2018-03-17 16:20:06], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [113][000/127]   Time 0.362 (0.362)   Data 0.327 (0.327)   Loss 0.6640 (0.6640)   Prec@1 75.000 (75.000)   Prec@5 100.000 (100.000)   [2018-03-17 16:20:06]
  **Train** Prec@1 79.247 Prec@5 98.093 Error@1 20.753
  **VAL** Prec@1 91.854 Prec@5 99.860 Error@1 8.146

==>>[2018-03-17 16:21:02] [Epoch=114/300] [Need: 02:52:32] [learning_rate=0.0002] [Best : Accuracy=93.12, Error=6.88]

==>>Epoch=[114/300]], [2018-03-17 16:21:02], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [114][000/127]   Time 0.384 (0.384)   Data 0.348 (0.348)   Loss 0.5824 (0.5824)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 16:21:02]
  **Train** Prec@1 79.445 Prec@5 98.118 Error@1 20.555
  **VAL** Prec@1 91.573 Prec@5 99.719 Error@1 8.427

==>>[2018-03-17 16:21:57] [Epoch=115/300] [Need: 02:51:37] [learning_rate=0.0002] [Best : Accuracy=93.12, Error=6.88]

==>>Epoch=[115/300]], [2018-03-17 16:21:57], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [115][000/127]   Time 0.345 (0.345)   Data 0.310 (0.310)   Loss 0.4538 (0.4538)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 16:21:58]
  **Train** Prec@1 80.213 Prec@5 98.167 Error@1 19.787
  **VAL** Prec@1 93.539 Prec@5 99.719 Error@1 6.461

==>>[2018-03-17 16:22:53] [Epoch=116/300] [Need: 02:50:41] [learning_rate=0.0002] [Best : Accuracy=93.54, Error=6.46]

==>>Epoch=[116/300]], [2018-03-17 16:22:53], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [116][000/127]   Time 0.322 (0.322)   Data 0.286 (0.286)   Loss 0.8550 (0.8550)   Prec@1 75.000 (75.000)   Prec@5 96.875 (96.875)   [2018-03-17 16:22:53]
  **Train** Prec@1 80.535 Prec@5 98.539 Error@1 19.465
  **VAL** Prec@1 90.730 Prec@5 99.860 Error@1 9.270

==>>[2018-03-17 16:23:48] [Epoch=117/300] [Need: 02:49:45] [learning_rate=0.0002] [Best : Accuracy=93.54, Error=6.46]

==>>Epoch=[117/300]], [2018-03-17 16:23:48], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [117][000/127]   Time 0.365 (0.365)   Data 0.330 (0.330)   Loss 0.4059 (0.4059)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:23:49]
  **Train** Prec@1 79.321 Prec@5 98.068 Error@1 20.679
  **VAL** Prec@1 92.135 Prec@5 99.719 Error@1 7.865

==>>[2018-03-17 16:24:44] [Epoch=118/300] [Need: 02:48:49] [learning_rate=0.0002] [Best : Accuracy=93.54, Error=6.46]

==>>Epoch=[118/300]], [2018-03-17 16:24:44], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [118][000/127]   Time 0.283 (0.283)   Data 0.246 (0.246)   Loss 0.6522 (0.6522)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 16:24:44]
  **Train** Prec@1 78.257 Prec@5 98.217 Error@1 21.743
  **VAL** Prec@1 90.590 Prec@5 99.860 Error@1 9.410

==>>[2018-03-17 16:25:40] [Epoch=119/300] [Need: 02:47:54] [learning_rate=0.0002] [Best : Accuracy=93.54, Error=6.46]

==>>Epoch=[119/300]], [2018-03-17 16:25:40], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [119][000/127]   Time 0.379 (0.379)   Data 0.343 (0.343)   Loss 0.3736 (0.3736)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 16:25:40]
  **Train** Prec@1 79.569 Prec@5 98.167 Error@1 20.431
  **VAL** Prec@1 92.837 Prec@5 99.860 Error@1 7.163

==>>[2018-03-17 16:26:37] [Epoch=120/300] [Need: 02:47:00] [learning_rate=0.0002] [Best : Accuracy=93.54, Error=6.46]

==>>Epoch=[120/300]], [2018-03-17 16:26:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [120][000/127]   Time 0.323 (0.323)   Data 0.287 (0.287)   Loss 0.7741 (0.7741)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 16:26:37]
  **Train** Prec@1 80.659 Prec@5 98.465 Error@1 19.341
  **VAL** Prec@1 86.798 Prec@5 99.719 Error@1 13.202

==>>[2018-03-17 16:27:33] [Epoch=121/300] [Need: 02:46:05] [learning_rate=0.0002] [Best : Accuracy=93.54, Error=6.46]

==>>Epoch=[121/300]], [2018-03-17 16:27:33], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [121][000/127]   Time 0.506 (0.506)   Data 0.471 (0.471)   Loss 0.4227 (0.4227)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:27:34]
  **Train** Prec@1 80.609 Prec@5 98.465 Error@1 19.391
  **VAL** Prec@1 93.820 Prec@5 99.860 Error@1 6.180

==>>[2018-03-17 16:28:29] [Epoch=122/300] [Need: 02:45:10] [learning_rate=0.0002] [Best : Accuracy=93.82, Error=6.18]

==>>Epoch=[122/300]], [2018-03-17 16:28:29], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [122][000/127]   Time 0.496 (0.496)   Data 0.457 (0.457)   Loss 0.3491 (0.3491)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:28:29]
  **Train** Prec@1 80.560 Prec@5 98.291 Error@1 19.440
  **VAL** Prec@1 92.697 Prec@5 99.860 Error@1 7.303

==>>[2018-03-17 16:29:25] [Epoch=123/300] [Need: 02:44:15] [learning_rate=0.0002] [Best : Accuracy=93.82, Error=6.18]

==>>Epoch=[123/300]], [2018-03-17 16:29:25], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [123][000/127]   Time 0.374 (0.374)   Data 0.339 (0.339)   Loss 0.6050 (0.6050)   Prec@1 81.250 (81.250)   Prec@5 96.875 (96.875)   [2018-03-17 16:29:25]
  **Train** Prec@1 81.105 Prec@5 98.341 Error@1 18.895
  **VAL** Prec@1 91.152 Prec@5 99.719 Error@1 8.848

==>>[2018-03-17 16:30:21] [Epoch=124/300] [Need: 02:43:20] [learning_rate=0.0002] [Best : Accuracy=93.82, Error=6.18]

==>>Epoch=[124/300]], [2018-03-17 16:30:21], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [124][000/127]   Time 0.247 (0.247)   Data 0.209 (0.209)   Loss 0.6323 (0.6323)   Prec@1 75.000 (75.000)   Prec@5 100.000 (100.000)   [2018-03-17 16:30:21]
  **Train** Prec@1 79.866 Prec@5 98.366 Error@1 20.134
  **VAL** Prec@1 92.556 Prec@5 99.579 Error@1 7.444

==>>[2018-03-17 16:31:17] [Epoch=125/300] [Need: 02:42:24] [learning_rate=0.0002] [Best : Accuracy=93.82, Error=6.18]

==>>Epoch=[125/300]], [2018-03-17 16:31:17], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [125][000/127]   Time 0.310 (0.310)   Data 0.274 (0.274)   Loss 0.5180 (0.5180)   Prec@1 75.000 (75.000)   Prec@5 96.875 (96.875)   [2018-03-17 16:31:17]
  **Train** Prec@1 80.584 Prec@5 98.440 Error@1 19.416
  **VAL** Prec@1 93.961 Prec@5 99.719 Error@1 6.039

==>>[2018-03-17 16:32:13] [Epoch=126/300] [Need: 02:41:29] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[126/300]], [2018-03-17 16:32:13], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [126][000/127]   Time 0.400 (0.400)   Data 0.363 (0.363)   Loss 0.8613 (0.8613)   Prec@1 71.875 (71.875)   Prec@5 96.875 (96.875)   [2018-03-17 16:32:14]
  **Train** Prec@1 79.842 Prec@5 98.242 Error@1 20.158
  **VAL** Prec@1 92.416 Prec@5 99.579 Error@1 7.584

==>>[2018-03-17 16:33:09] [Epoch=127/300] [Need: 02:40:34] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[127/300]], [2018-03-17 16:33:09], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [127][000/127]   Time 0.344 (0.344)   Data 0.308 (0.308)   Loss 0.7780 (0.7780)   Prec@1 75.000 (75.000)   Prec@5 100.000 (100.000)   [2018-03-17 16:33:09]
  **Train** Prec@1 80.312 Prec@5 98.341 Error@1 19.688
  **VAL** Prec@1 91.713 Prec@5 99.860 Error@1 8.287

==>>[2018-03-17 16:34:04] [Epoch=128/300] [Need: 02:39:38] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[128/300]], [2018-03-17 16:34:04], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [128][000/127]   Time 0.454 (0.454)   Data 0.417 (0.417)   Loss 0.4093 (0.4093)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 16:34:05]
  **Train** Prec@1 80.312 Prec@5 98.266 Error@1 19.688
  **VAL** Prec@1 89.466 Prec@5 99.860 Error@1 10.534

==>>[2018-03-17 16:35:00] [Epoch=129/300] [Need: 02:38:42] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[129/300]], [2018-03-17 16:35:00], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [129][000/127]   Time 0.458 (0.458)   Data 0.421 (0.421)   Loss 0.5938 (0.5938)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 16:35:01]
  **Train** Prec@1 81.253 Prec@5 98.440 Error@1 18.747
  **VAL** Prec@1 92.556 Prec@5 99.860 Error@1 7.444

==>>[2018-03-17 16:35:56] [Epoch=130/300] [Need: 02:37:47] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[130/300]], [2018-03-17 16:35:56], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [130][000/127]   Time 0.468 (0.468)   Data 0.431 (0.431)   Loss 0.5266 (0.5266)   Prec@1 90.625 (90.625)   Prec@5 96.875 (96.875)   [2018-03-17 16:35:56]
  **Train** Prec@1 80.287 Prec@5 98.514 Error@1 19.713
  **VAL** Prec@1 92.978 Prec@5 99.719 Error@1 7.022

==>>[2018-03-17 16:36:51] [Epoch=131/300] [Need: 02:36:51] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[131/300]], [2018-03-17 16:36:51], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [131][000/127]   Time 0.450 (0.450)   Data 0.413 (0.413)   Loss 0.7611 (0.7611)   Prec@1 87.500 (87.500)   Prec@5 96.875 (96.875)   [2018-03-17 16:36:52]
  **Train** Prec@1 79.718 Prec@5 98.539 Error@1 20.282
  **VAL** Prec@1 91.573 Prec@5 99.719 Error@1 8.427

==>>[2018-03-17 16:37:47] [Epoch=132/300] [Need: 02:35:55] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[132/300]], [2018-03-17 16:37:47], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [132][000/127]   Time 0.331 (0.331)   Data 0.294 (0.294)   Loss 0.7174 (0.7174)   Prec@1 78.125 (78.125)   Prec@5 93.750 (93.750)   [2018-03-17 16:37:47]
  **Train** Prec@1 82.368 Prec@5 98.712 Error@1 17.632
  **VAL** Prec@1 91.994 Prec@5 99.860 Error@1 8.006

==>>[2018-03-17 16:38:43] [Epoch=133/300] [Need: 02:34:59] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[133/300]], [2018-03-17 16:38:43], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [133][000/127]   Time 0.383 (0.383)   Data 0.345 (0.345)   Loss 0.4204 (0.4204)   Prec@1 90.625 (90.625)   Prec@5 96.875 (96.875)   [2018-03-17 16:38:43]
  **Train** Prec@1 80.733 Prec@5 98.588 Error@1 19.267
  **VAL** Prec@1 92.556 Prec@5 99.719 Error@1 7.444

==>>[2018-03-17 16:39:38] [Epoch=134/300] [Need: 02:34:04] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[134/300]], [2018-03-17 16:39:38], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [134][000/127]   Time 0.281 (0.281)   Data 0.246 (0.246)   Loss 0.4498 (0.4498)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 16:39:39]
  **Train** Prec@1 81.996 Prec@5 98.663 Error@1 18.004
  **VAL** Prec@1 91.433 Prec@5 99.719 Error@1 8.567

==>>[2018-03-17 16:40:34] [Epoch=135/300] [Need: 02:33:08] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[135/300]], [2018-03-17 16:40:34], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [135][000/127]   Time 0.369 (0.369)   Data 0.332 (0.332)   Loss 0.5524 (0.5524)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 16:40:34]
  **Train** Prec@1 81.030 Prec@5 98.266 Error@1 18.970
  **VAL** Prec@1 92.135 Prec@5 99.860 Error@1 7.865

==>>[2018-03-17 16:41:30] [Epoch=136/300] [Need: 02:32:12] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[136/300]], [2018-03-17 16:41:30], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [136][000/127]   Time 0.494 (0.494)   Data 0.457 (0.457)   Loss 0.3481 (0.3481)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:41:30]
  **Train** Prec@1 80.114 Prec@5 98.068 Error@1 19.886
  **VAL** Prec@1 90.309 Prec@5 99.860 Error@1 9.691

==>>[2018-03-17 16:42:25] [Epoch=137/300] [Need: 02:31:16] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[137/300]], [2018-03-17 16:42:25], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [137][000/127]   Time 0.359 (0.359)   Data 0.322 (0.322)   Loss 0.4313 (0.4313)   Prec@1 87.500 (87.500)   Prec@5 96.875 (96.875)   [2018-03-17 16:42:26]
  **Train** Prec@1 81.327 Prec@5 98.440 Error@1 18.673
  **VAL** Prec@1 92.416 Prec@5 99.860 Error@1 7.584

==>>[2018-03-17 16:43:21] [Epoch=138/300] [Need: 02:30:21] [learning_rate=0.0002] [Best : Accuracy=93.96, Error=6.04]

==>>Epoch=[138/300]], [2018-03-17 16:43:21], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [138][000/127]   Time 0.358 (0.358)   Data 0.321 (0.321)   Loss 0.8385 (0.8385)   Prec@1 71.875 (71.875)   Prec@5 100.000 (100.000)   [2018-03-17 16:43:21]
  **Train** Prec@1 81.204 Prec@5 98.687 Error@1 18.796
  **VAL** Prec@1 94.522 Prec@5 99.860 Error@1 5.478

==>>[2018-03-17 16:44:17] [Epoch=139/300] [Need: 02:29:25] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[139/300]], [2018-03-17 16:44:17], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [139][000/127]   Time 0.417 (0.417)   Data 0.380 (0.380)   Loss 0.3890 (0.3890)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 16:44:17]
  **Train** Prec@1 80.956 Prec@5 98.366 Error@1 19.044
  **VAL** Prec@1 90.590 Prec@5 99.719 Error@1 9.410

==>>[2018-03-17 16:45:12] [Epoch=140/300] [Need: 02:28:29] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[140/300]], [2018-03-17 16:45:12], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [140][000/127]   Time 0.326 (0.326)   Data 0.289 (0.289)   Loss 0.3676 (0.3676)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 16:45:13]
  **Train** Prec@1 81.600 Prec@5 98.663 Error@1 18.400
  **VAL** Prec@1 93.539 Prec@5 99.860 Error@1 6.461

==>>[2018-03-17 16:46:08] [Epoch=141/300] [Need: 02:27:34] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[141/300]], [2018-03-17 16:46:08], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [141][000/127]   Time 0.293 (0.293)   Data 0.256 (0.256)   Loss 0.7039 (0.7039)   Prec@1 68.750 (68.750)   Prec@5 96.875 (96.875)   [2018-03-17 16:46:09]
  **Train** Prec@1 80.857 Prec@5 98.588 Error@1 19.143
  **VAL** Prec@1 92.978 Prec@5 99.860 Error@1 7.022

==>>[2018-03-17 16:47:04] [Epoch=142/300] [Need: 02:26:38] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[142/300]], [2018-03-17 16:47:04], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [142][000/127]   Time 0.473 (0.473)   Data 0.436 (0.436)   Loss 0.5397 (0.5397)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 16:47:04]
  **Train** Prec@1 81.773 Prec@5 98.762 Error@1 18.227
  **VAL** Prec@1 92.416 Prec@5 100.000 Error@1 7.584

==>>[2018-03-17 16:48:00] [Epoch=143/300] [Need: 02:25:42] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[143/300]], [2018-03-17 16:48:00], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [143][000/127]   Time 0.348 (0.348)   Data 0.312 (0.312)   Loss 0.4118 (0.4118)   Prec@1 87.500 (87.500)   Prec@5 96.875 (96.875)   [2018-03-17 16:48:00]
  **Train** Prec@1 80.584 Prec@5 98.539 Error@1 19.416
  **VAL** Prec@1 93.258 Prec@5 99.860 Error@1 6.742

==>>[2018-03-17 16:48:55] [Epoch=144/300] [Need: 02:24:47] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[144/300]], [2018-03-17 16:48:55], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [144][000/127]   Time 0.284 (0.284)   Data 0.247 (0.247)   Loss 0.7307 (0.7307)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 16:48:55]
  **Train** Prec@1 81.748 Prec@5 98.564 Error@1 18.252
  **VAL** Prec@1 91.573 Prec@5 99.719 Error@1 8.427

==>>[2018-03-17 16:49:51] [Epoch=145/300] [Need: 02:23:51] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[145/300]], [2018-03-17 16:49:51], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [145][000/127]   Time 0.384 (0.384)   Data 0.347 (0.347)   Loss 0.5234 (0.5234)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 16:49:51]
  **Train** Prec@1 81.501 Prec@5 98.613 Error@1 18.499
  **VAL** Prec@1 91.292 Prec@5 99.719 Error@1 8.708

==>>[2018-03-17 16:50:46] [Epoch=146/300] [Need: 02:22:55] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[146/300]], [2018-03-17 16:50:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [146][000/127]   Time 0.295 (0.295)   Data 0.259 (0.259)   Loss 0.5704 (0.5704)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:50:47]
  **Train** Prec@1 82.046 Prec@5 98.762 Error@1 17.954
  **VAL** Prec@1 92.978 Prec@5 99.860 Error@1 7.022

==>>[2018-03-17 16:51:42] [Epoch=147/300] [Need: 02:21:59] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[147/300]], [2018-03-17 16:51:42], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [147][000/127]   Time 0.282 (0.282)   Data 0.247 (0.247)   Loss 0.3030 (0.3030)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-03-17 16:51:42]
  **Train** Prec@1 81.179 Prec@5 98.588 Error@1 18.821
  **VAL** Prec@1 91.433 Prec@5 100.000 Error@1 8.567

==>>[2018-03-17 16:52:38] [Epoch=148/300] [Need: 02:21:04] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[148/300]], [2018-03-17 16:52:38], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [148][000/127]   Time 0.247 (0.247)   Data 0.210 (0.210)   Loss 0.6219 (0.6219)   Prec@1 87.500 (87.500)   Prec@5 96.875 (96.875)   [2018-03-17 16:52:38]
  **Train** Prec@1 81.526 Prec@5 98.465 Error@1 18.474
  **VAL** Prec@1 90.590 Prec@5 99.860 Error@1 9.410

==>>[2018-03-17 16:53:33] [Epoch=149/300] [Need: 02:20:08] [learning_rate=0.0002] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[149/300]], [2018-03-17 16:53:33], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [149][000/127]   Time 0.337 (0.337)   Data 0.302 (0.302)   Loss 0.3071 (0.3071)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:53:34]
  **Train** Prec@1 81.872 Prec@5 98.489 Error@1 18.128
  **VAL** Prec@1 92.275 Prec@5 100.000 Error@1 7.725

==>>[2018-03-17 16:54:29] [Epoch=150/300] [Need: 02:19:12] [learning_rate=0.0000] [Best : Accuracy=94.52, Error=5.48]

==>>Epoch=[150/300]], [2018-03-17 16:54:29], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [150][000/127]   Time 0.301 (0.301)   Data 0.264 (0.264)   Loss 0.3624 (0.3624)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 16:54:29]
  **Train** Prec@1 82.491 Prec@5 98.539 Error@1 17.509
  **VAL** Prec@1 94.803 Prec@5 100.000 Error@1 5.197

==>>[2018-03-17 16:55:25] [Epoch=151/300] [Need: 02:18:17] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[151/300]], [2018-03-17 16:55:25], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [151][000/127]   Time 0.361 (0.361)   Data 0.324 (0.324)   Loss 0.2764 (0.2764)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-03-17 16:55:25]
  **Train** Prec@1 83.358 Prec@5 98.836 Error@1 16.642
  **VAL** Prec@1 94.101 Prec@5 100.000 Error@1 5.899

==>>[2018-03-17 16:56:21] [Epoch=152/300] [Need: 02:17:21] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[152/300]], [2018-03-17 16:56:21], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [152][000/127]   Time 0.332 (0.332)   Data 0.296 (0.296)   Loss 0.6222 (0.6222)   Prec@1 71.875 (71.875)   Prec@5 100.000 (100.000)   [2018-03-17 16:56:21]
  **Train** Prec@1 83.259 Prec@5 98.613 Error@1 16.741
  **VAL** Prec@1 93.961 Prec@5 99.860 Error@1 6.039

==>>[2018-03-17 16:57:16] [Epoch=153/300] [Need: 02:16:25] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[153/300]], [2018-03-17 16:57:16], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [153][000/127]   Time 0.323 (0.323)   Data 0.286 (0.286)   Loss 0.4525 (0.4525)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 16:57:17]
  **Train** Prec@1 83.680 Prec@5 98.985 Error@1 16.320
  **VAL** Prec@1 93.820 Prec@5 100.000 Error@1 6.180

==>>[2018-03-17 16:58:12] [Epoch=154/300] [Need: 02:15:30] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[154/300]], [2018-03-17 16:58:12], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [154][000/127]   Time 0.300 (0.300)   Data 0.265 (0.265)   Loss 0.3656 (0.3656)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 16:58:12]
  **Train** Prec@1 84.101 Prec@5 98.514 Error@1 15.899
  **VAL** Prec@1 94.382 Prec@5 100.000 Error@1 5.618

==>>[2018-03-17 16:59:08] [Epoch=155/300] [Need: 02:14:34] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[155/300]], [2018-03-17 16:59:08], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [155][000/127]   Time 0.465 (0.465)   Data 0.427 (0.427)   Loss 0.2916 (0.2916)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-03-17 16:59:08]
  **Train** Prec@1 83.680 Prec@5 98.514 Error@1 16.320
  **VAL** Prec@1 94.242 Prec@5 100.000 Error@1 5.758

==>>[2018-03-17 17:00:03] [Epoch=156/300] [Need: 02:13:38] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[156/300]], [2018-03-17 17:00:03], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [156][000/127]   Time 0.285 (0.285)   Data 0.249 (0.249)   Loss 0.4473 (0.4473)   Prec@1 84.375 (84.375)   Prec@5 96.875 (96.875)   [2018-03-17 17:00:03]
  **Train** Prec@1 83.631 Prec@5 98.489 Error@1 16.369
  **VAL** Prec@1 93.820 Prec@5 99.860 Error@1 6.180

==>>[2018-03-17 17:00:59] [Epoch=157/300] [Need: 02:12:42] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[157/300]], [2018-03-17 17:00:59], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [157][000/127]   Time 0.362 (0.362)   Data 0.326 (0.326)   Loss 0.5140 (0.5140)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 17:00:59]
  **Train** Prec@1 84.547 Prec@5 98.737 Error@1 15.453
  **VAL** Prec@1 93.820 Prec@5 99.860 Error@1 6.180

==>>[2018-03-17 17:01:54] [Epoch=158/300] [Need: 02:11:47] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[158/300]], [2018-03-17 17:01:54], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [158][000/127]   Time 0.405 (0.405)   Data 0.368 (0.368)   Loss 0.3042 (0.3042)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 17:01:55]
  **Train** Prec@1 83.730 Prec@5 98.687 Error@1 16.270
  **VAL** Prec@1 94.101 Prec@5 99.860 Error@1 5.899

==>>[2018-03-17 17:02:50] [Epoch=159/300] [Need: 02:10:51] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[159/300]], [2018-03-17 17:02:50], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [159][000/127]   Time 0.283 (0.283)   Data 0.247 (0.247)   Loss 0.6859 (0.6859)   Prec@1 75.000 (75.000)   Prec@5 96.875 (96.875)   [2018-03-17 17:02:50]
  **Train** Prec@1 84.349 Prec@5 98.787 Error@1 15.651
  **VAL** Prec@1 94.382 Prec@5 100.000 Error@1 5.618

==>>[2018-03-17 17:03:46] [Epoch=160/300] [Need: 02:09:55] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[160/300]], [2018-03-17 17:03:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [160][000/127]   Time 0.399 (0.399)   Data 0.364 (0.364)   Loss 0.3021 (0.3021)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 17:03:46]
  **Train** Prec@1 84.646 Prec@5 98.787 Error@1 15.354
  **VAL** Prec@1 93.399 Prec@5 100.000 Error@1 6.601

==>>[2018-03-17 17:04:41] [Epoch=161/300] [Need: 02:09:00] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[161/300]], [2018-03-17 17:04:41], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [161][000/127]   Time 0.417 (0.417)   Data 0.380 (0.380)   Loss 0.5358 (0.5358)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 17:04:42]
  **Train** Prec@1 83.185 Prec@5 98.217 Error@1 16.815
  **VAL** Prec@1 94.242 Prec@5 100.000 Error@1 5.758

==>>[2018-03-17 17:05:37] [Epoch=162/300] [Need: 02:08:04] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[162/300]], [2018-03-17 17:05:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [162][000/127]   Time 0.276 (0.276)   Data 0.241 (0.241)   Loss 1.0118 (1.0118)   Prec@1 65.625 (65.625)   Prec@5 93.750 (93.750)   [2018-03-17 17:05:37]
  **Train** Prec@1 84.770 Prec@5 98.663 Error@1 15.230
  **VAL** Prec@1 94.803 Prec@5 99.860 Error@1 5.197

==>>[2018-03-17 17:06:33] [Epoch=163/300] [Need: 02:07:08] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[163/300]], [2018-03-17 17:06:33], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [163][000/127]   Time 0.348 (0.348)   Data 0.311 (0.311)   Loss 0.4990 (0.4990)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 17:06:33]
  **Train** Prec@1 84.522 Prec@5 98.836 Error@1 15.478
  **VAL** Prec@1 94.522 Prec@5 100.000 Error@1 5.478

==>>[2018-03-17 17:07:28] [Epoch=164/300] [Need: 02:06:12] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[164/300]], [2018-03-17 17:07:28], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [164][000/127]   Time 0.340 (0.340)   Data 0.304 (0.304)   Loss 0.3923 (0.3923)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:07:29]
  **Train** Prec@1 84.745 Prec@5 99.034 Error@1 15.255
  **VAL** Prec@1 94.242 Prec@5 99.860 Error@1 5.758

==>>[2018-03-17 17:08:24] [Epoch=165/300] [Need: 02:05:17] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[165/300]], [2018-03-17 17:08:24], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [165][000/127]   Time 0.492 (0.492)   Data 0.457 (0.457)   Loss 0.3462 (0.3462)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-03-17 17:08:24]
  **Train** Prec@1 84.695 Prec@5 98.811 Error@1 15.305
  **VAL** Prec@1 93.680 Prec@5 100.000 Error@1 6.320

==>>[2018-03-17 17:09:20] [Epoch=166/300] [Need: 02:04:21] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[166/300]], [2018-03-17 17:09:20], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [166][000/127]   Time 0.478 (0.478)   Data 0.442 (0.442)   Loss 0.6732 (0.6732)   Prec@1 78.125 (78.125)   Prec@5 90.625 (90.625)   [2018-03-17 17:09:20]
  **Train** Prec@1 84.027 Prec@5 98.564 Error@1 15.973
  **VAL** Prec@1 94.382 Prec@5 100.000 Error@1 5.618

==>>[2018-03-17 17:10:15] [Epoch=167/300] [Need: 02:03:25] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[167/300]], [2018-03-17 17:10:15], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [167][000/127]   Time 0.422 (0.422)   Data 0.385 (0.385)   Loss 0.1607 (0.1607)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-03-17 17:10:16]
  **Train** Prec@1 85.017 Prec@5 98.588 Error@1 14.983
  **VAL** Prec@1 94.101 Prec@5 100.000 Error@1 5.899

==>>[2018-03-17 17:11:11] [Epoch=168/300] [Need: 02:02:30] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[168/300]], [2018-03-17 17:11:11], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [168][000/127]   Time 0.438 (0.438)   Data 0.400 (0.400)   Loss 0.4941 (0.4941)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 17:11:11]
  **Train** Prec@1 84.349 Prec@5 98.836 Error@1 15.651
  **VAL** Prec@1 94.101 Prec@5 100.000 Error@1 5.899

==>>[2018-03-17 17:12:06] [Epoch=169/300] [Need: 02:01:34] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[169/300]], [2018-03-17 17:12:06], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [169][000/127]   Time 0.351 (0.351)   Data 0.315 (0.315)   Loss 0.1474 (0.1474)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-03-17 17:12:07]
  **Train** Prec@1 84.844 Prec@5 98.960 Error@1 15.156
  **VAL** Prec@1 93.399 Prec@5 100.000 Error@1 6.601

==>>[2018-03-17 17:13:02] [Epoch=170/300] [Need: 02:00:38] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[170/300]], [2018-03-17 17:13:02], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [170][000/127]   Time 0.439 (0.439)   Data 0.401 (0.401)   Loss 0.3116 (0.3116)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 17:13:03]
  **Train** Prec@1 85.141 Prec@5 98.811 Error@1 14.859
  **VAL** Prec@1 93.258 Prec@5 100.000 Error@1 6.742

==>>[2018-03-17 17:13:58] [Epoch=171/300] [Need: 01:59:42] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[171/300]], [2018-03-17 17:13:58], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [171][000/127]   Time 0.389 (0.389)   Data 0.353 (0.353)   Loss 0.6386 (0.6386)   Prec@1 75.000 (75.000)   Prec@5 96.875 (96.875)   [2018-03-17 17:13:58]
  **Train** Prec@1 84.200 Prec@5 98.613 Error@1 15.800
  **VAL** Prec@1 94.101 Prec@5 100.000 Error@1 5.899

==>>[2018-03-17 17:14:53] [Epoch=172/300] [Need: 01:58:47] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[172/300]], [2018-03-17 17:14:53], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [172][000/127]   Time 0.304 (0.304)   Data 0.267 (0.267)   Loss 0.5651 (0.5651)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 17:14:54]
  **Train** Prec@1 84.621 Prec@5 98.762 Error@1 15.379
  **VAL** Prec@1 93.680 Prec@5 100.000 Error@1 6.320

==>>[2018-03-17 17:15:49] [Epoch=173/300] [Need: 01:57:51] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[173/300]], [2018-03-17 17:15:49], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [173][000/127]   Time 0.288 (0.288)   Data 0.253 (0.253)   Loss 0.3854 (0.3854)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:15:49]
  **Train** Prec@1 84.547 Prec@5 98.787 Error@1 15.453
  **VAL** Prec@1 94.382 Prec@5 100.000 Error@1 5.618

==>>[2018-03-17 17:16:45] [Epoch=174/300] [Need: 01:56:55] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[174/300]], [2018-03-17 17:16:45], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [174][000/127]   Time 0.430 (0.430)   Data 0.395 (0.395)   Loss 0.5448 (0.5448)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 17:16:45]
  **Train** Prec@1 84.646 Prec@5 98.910 Error@1 15.354
  **VAL** Prec@1 93.961 Prec@5 100.000 Error@1 6.039

==>>[2018-03-17 17:17:40] [Epoch=175/300] [Need: 01:56:00] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[175/300]], [2018-03-17 17:17:40], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [175][000/127]   Time 0.278 (0.278)   Data 0.241 (0.241)   Loss 0.3019 (0.3019)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 17:17:41]
  **Train** Prec@1 84.770 Prec@5 98.465 Error@1 15.230
  **VAL** Prec@1 94.382 Prec@5 100.000 Error@1 5.618

==>>[2018-03-17 17:18:36] [Epoch=176/300] [Need: 01:55:04] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[176/300]], [2018-03-17 17:18:36], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [176][000/127]   Time 0.262 (0.262)   Data 0.225 (0.225)   Loss 0.6768 (0.6768)   Prec@1 81.250 (81.250)   Prec@5 96.875 (96.875)   [2018-03-17 17:18:36]
  **Train** Prec@1 84.844 Prec@5 98.836 Error@1 15.156
  **VAL** Prec@1 94.382 Prec@5 100.000 Error@1 5.618

==>>[2018-03-17 17:19:32] [Epoch=177/300] [Need: 01:54:08] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[177/300]], [2018-03-17 17:19:32], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [177][000/127]   Time 0.332 (0.332)   Data 0.295 (0.295)   Loss 0.3863 (0.3863)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 17:19:32]
  **Train** Prec@1 84.274 Prec@5 98.836 Error@1 15.726
  **VAL** Prec@1 93.820 Prec@5 100.000 Error@1 6.180

==>>[2018-03-17 17:20:27] [Epoch=178/300] [Need: 01:53:12] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[178/300]], [2018-03-17 17:20:27], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [178][000/127]   Time 0.287 (0.287)   Data 0.251 (0.251)   Loss 0.4773 (0.4773)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 17:20:28]
  **Train** Prec@1 84.349 Prec@5 98.787 Error@1 15.651
  **VAL** Prec@1 93.680 Prec@5 100.000 Error@1 6.320

==>>[2018-03-17 17:21:23] [Epoch=179/300] [Need: 01:52:17] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[179/300]], [2018-03-17 17:21:23], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [179][000/127]   Time 0.467 (0.467)   Data 0.430 (0.430)   Loss 0.4159 (0.4159)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 17:21:23]
  **Train** Prec@1 84.745 Prec@5 99.034 Error@1 15.255
  **VAL** Prec@1 94.382 Prec@5 99.860 Error@1 5.618

==>>[2018-03-17 17:22:18] [Epoch=180/300] [Need: 01:51:21] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[180/300]], [2018-03-17 17:22:18], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [180][000/127]   Time 0.362 (0.362)   Data 0.325 (0.325)   Loss 0.4493 (0.4493)   Prec@1 84.375 (84.375)   Prec@5 96.875 (96.875)   [2018-03-17 17:22:19]
  **Train** Prec@1 84.745 Prec@5 99.059 Error@1 15.255
  **VAL** Prec@1 94.522 Prec@5 100.000 Error@1 5.478

==>>[2018-03-17 17:23:14] [Epoch=181/300] [Need: 01:50:25] [learning_rate=0.0000] [Best : Accuracy=94.80, Error=5.20]

==>>Epoch=[181/300]], [2018-03-17 17:23:14], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [181][000/127]   Time 0.321 (0.321)   Data 0.285 (0.285)   Loss 0.7909 (0.7909)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 17:23:15]
  **Train** Prec@1 84.423 Prec@5 98.588 Error@1 15.577
  **VAL** Prec@1 95.084 Prec@5 100.000 Error@1 4.916

==>>[2018-03-17 17:24:15] [Epoch=182/300] [Need: 01:49:30] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[182/300]], [2018-03-17 17:24:15], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [182][000/127]   Time 0.563 (0.563)   Data 0.487 (0.487)   Loss 0.3604 (0.3604)   Prec@1 87.500 (87.500)   Prec@5 96.875 (96.875)   [2018-03-17 17:24:16]
  **Train** Prec@1 84.621 Prec@5 98.985 Error@1 15.379
  **VAL** Prec@1 94.242 Prec@5 100.000 Error@1 5.758

==>>[2018-03-17 17:25:11] [Epoch=183/300] [Need: 01:48:37] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[183/300]], [2018-03-17 17:25:11], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [183][000/127]   Time 0.293 (0.293)   Data 0.256 (0.256)   Loss 0.4208 (0.4208)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-03-17 17:25:11]
  **Train** Prec@1 84.918 Prec@5 98.712 Error@1 15.082
  **VAL** Prec@1 93.820 Prec@5 100.000 Error@1 6.180

==>>[2018-03-17 17:26:06] [Epoch=184/300] [Need: 01:47:41] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[184/300]], [2018-03-17 17:26:06], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [184][000/127]   Time 0.336 (0.336)   Data 0.301 (0.301)   Loss 0.2941 (0.2941)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 17:26:07]
  **Train** Prec@1 84.671 Prec@5 98.712 Error@1 15.329
  **VAL** Prec@1 94.382 Prec@5 99.860 Error@1 5.618

==>>[2018-03-17 17:27:02] [Epoch=185/300] [Need: 01:46:46] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[185/300]], [2018-03-17 17:27:02], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [185][000/127]   Time 0.306 (0.306)   Data 0.271 (0.271)   Loss 0.4258 (0.4258)   Prec@1 84.375 (84.375)   Prec@5 96.875 (96.875)   [2018-03-17 17:27:02]
  **Train** Prec@1 85.042 Prec@5 98.787 Error@1 14.958
  **VAL** Prec@1 93.820 Prec@5 100.000 Error@1 6.180

==>>[2018-03-17 17:27:57] [Epoch=186/300] [Need: 01:45:50] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[186/300]], [2018-03-17 17:27:57], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [186][000/127]   Time 0.347 (0.347)   Data 0.312 (0.312)   Loss 0.7054 (0.7054)   Prec@1 71.875 (71.875)   Prec@5 100.000 (100.000)   [2018-03-17 17:27:58]
  **Train** Prec@1 84.225 Prec@5 98.737 Error@1 15.775
  **VAL** Prec@1 94.803 Prec@5 99.860 Error@1 5.197

==>>[2018-03-17 17:28:53] [Epoch=187/300] [Need: 01:44:54] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[187/300]], [2018-03-17 17:28:53], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [187][000/127]   Time 0.441 (0.441)   Data 0.406 (0.406)   Loss 0.5510 (0.5510)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:28:53]
  **Train** Prec@1 84.423 Prec@5 98.935 Error@1 15.577
  **VAL** Prec@1 93.680 Prec@5 100.000 Error@1 6.320

==>>[2018-03-17 17:29:48] [Epoch=188/300] [Need: 01:43:58] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[188/300]], [2018-03-17 17:29:48], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [188][000/127]   Time 0.291 (0.291)   Data 0.255 (0.255)   Loss 0.3120 (0.3120)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 17:29:49]
  **Train** Prec@1 84.869 Prec@5 98.910 Error@1 15.131
  **VAL** Prec@1 94.522 Prec@5 99.860 Error@1 5.478

==>>[2018-03-17 17:30:44] [Epoch=189/300] [Need: 01:43:02] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[189/300]], [2018-03-17 17:30:44], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [189][000/127]   Time 0.377 (0.377)   Data 0.342 (0.342)   Loss 0.2615 (0.2615)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 17:30:44]
  **Train** Prec@1 84.720 Prec@5 98.910 Error@1 15.280
  **VAL** Prec@1 93.539 Prec@5 100.000 Error@1 6.461

==>>[2018-03-17 17:31:40] [Epoch=190/300] [Need: 01:42:07] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[190/300]], [2018-03-17 17:31:40], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [190][000/127]   Time 0.456 (0.456)   Data 0.420 (0.420)   Loss 0.5492 (0.5492)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2018-03-17 17:31:40]
  **Train** Prec@1 85.810 Prec@5 99.009 Error@1 14.190
  **VAL** Prec@1 94.382 Prec@5 100.000 Error@1 5.618

==>>[2018-03-17 17:32:35] [Epoch=191/300] [Need: 01:41:11] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[191/300]], [2018-03-17 17:32:35], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [191][000/127]   Time 0.289 (0.289)   Data 0.252 (0.252)   Loss 0.3053 (0.3053)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:32:35]
  **Train** Prec@1 84.918 Prec@5 98.638 Error@1 15.082
  **VAL** Prec@1 93.961 Prec@5 99.860 Error@1 6.039

==>>[2018-03-17 17:33:31] [Epoch=192/300] [Need: 01:40:15] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[192/300]], [2018-03-17 17:33:31], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [192][000/127]   Time 0.295 (0.295)   Data 0.259 (0.259)   Loss 0.6351 (0.6351)   Prec@1 81.250 (81.250)   Prec@5 96.875 (96.875)   [2018-03-17 17:33:31]
  **Train** Prec@1 85.116 Prec@5 99.133 Error@1 14.884
  **VAL** Prec@1 94.242 Prec@5 100.000 Error@1 5.758

==>>[2018-03-17 17:34:26] [Epoch=193/300] [Need: 01:39:19] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[193/300]], [2018-03-17 17:34:26], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [193][000/127]   Time 0.439 (0.439)   Data 0.404 (0.404)   Loss 0.3604 (0.3604)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 17:34:27]
  **Train** Prec@1 84.869 Prec@5 98.712 Error@1 15.131
  **VAL** Prec@1 94.522 Prec@5 99.860 Error@1 5.478

==>>[2018-03-17 17:35:22] [Epoch=194/300] [Need: 01:38:24] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[194/300]], [2018-03-17 17:35:22], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [194][000/127]   Time 0.349 (0.349)   Data 0.314 (0.314)   Loss 0.4152 (0.4152)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 17:35:22]
  **Train** Prec@1 85.240 Prec@5 98.910 Error@1 14.760
  **VAL** Prec@1 94.101 Prec@5 100.000 Error@1 5.899

==>>[2018-03-17 17:36:17] [Epoch=195/300] [Need: 01:37:28] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[195/300]], [2018-03-17 17:36:17], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [195][000/127]   Time 0.553 (0.553)   Data 0.518 (0.518)   Loss 0.3400 (0.3400)   Prec@1 90.625 (90.625)   Prec@5 96.875 (96.875)   [2018-03-17 17:36:18]
  **Train** Prec@1 84.720 Prec@5 98.886 Error@1 15.280
  **VAL** Prec@1 93.820 Prec@5 100.000 Error@1 6.180

==>>[2018-03-17 17:37:13] [Epoch=196/300] [Need: 01:36:32] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[196/300]], [2018-03-17 17:37:13], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [196][000/127]   Time 0.338 (0.338)   Data 0.303 (0.303)   Loss 0.2254 (0.2254)   Prec@1 93.750 (93.750)   Prec@5 96.875 (96.875)   [2018-03-17 17:37:13]
  **Train** Prec@1 85.587 Prec@5 98.935 Error@1 14.413
  **VAL** Prec@1 94.522 Prec@5 100.000 Error@1 5.478

==>>[2018-03-17 17:38:08] [Epoch=197/300] [Need: 01:35:36] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[197/300]], [2018-03-17 17:38:08], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [197][000/127]   Time 0.354 (0.354)   Data 0.319 (0.319)   Loss 0.5195 (0.5195)   Prec@1 84.375 (84.375)   Prec@5 96.875 (96.875)   [2018-03-17 17:38:09]
  **Train** Prec@1 84.052 Prec@5 98.886 Error@1 15.948
  **VAL** Prec@1 94.522 Prec@5 100.000 Error@1 5.478

==>>[2018-03-17 17:39:04] [Epoch=198/300] [Need: 01:34:40] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[198/300]], [2018-03-17 17:39:04], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [198][000/127]   Time 0.282 (0.282)   Data 0.246 (0.246)   Loss 0.3132 (0.3132)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:39:04]
  **Train** Prec@1 85.166 Prec@5 98.811 Error@1 14.834
  **VAL** Prec@1 94.803 Prec@5 100.000 Error@1 5.197

==>>[2018-03-17 17:40:00] [Epoch=199/300] [Need: 01:33:45] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[199/300]], [2018-03-17 17:40:00], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [199][000/127]   Time 0.332 (0.332)   Data 0.297 (0.297)   Loss 0.4221 (0.4221)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:40:00]
  **Train** Prec@1 84.398 Prec@5 98.886 Error@1 15.602
  **VAL** Prec@1 94.663 Prec@5 100.000 Error@1 5.337

==>>[2018-03-17 17:40:55] [Epoch=200/300] [Need: 01:32:49] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[200/300]], [2018-03-17 17:40:55], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [200][000/127]   Time 0.370 (0.370)   Data 0.333 (0.333)   Loss 0.3119 (0.3119)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:40:56]
  **Train** Prec@1 84.794 Prec@5 99.133 Error@1 15.206
  **VAL** Prec@1 94.522 Prec@5 99.860 Error@1 5.478

==>>[2018-03-17 17:41:51] [Epoch=201/300] [Need: 01:31:53] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[201/300]], [2018-03-17 17:41:51], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [201][000/127]   Time 0.456 (0.456)   Data 0.420 (0.420)   Loss 0.3272 (0.3272)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-03-17 17:41:51]
  **Train** Prec@1 84.423 Prec@5 98.489 Error@1 15.577
  **VAL** Prec@1 94.803 Prec@5 100.000 Error@1 5.197

==>>[2018-03-17 17:42:46] [Epoch=202/300] [Need: 01:30:57] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[202/300]], [2018-03-17 17:42:46], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [202][000/127]   Time 0.320 (0.320)   Data 0.283 (0.283)   Loss 0.4002 (0.4002)   Prec@1 87.500 (87.500)   Prec@5 96.875 (96.875)   [2018-03-17 17:42:47]
  **Train** Prec@1 85.414 Prec@5 98.613 Error@1 14.586
  **VAL** Prec@1 94.803 Prec@5 100.000 Error@1 5.197

==>>[2018-03-17 17:43:42] [Epoch=203/300] [Need: 01:30:02] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[203/300]], [2018-03-17 17:43:42], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [203][000/127]   Time 0.521 (0.521)   Data 0.485 (0.485)   Loss 0.5549 (0.5549)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:43:42]
  **Train** Prec@1 84.794 Prec@5 98.762 Error@1 15.206
  **VAL** Prec@1 94.944 Prec@5 100.000 Error@1 5.056

==>>[2018-03-17 17:44:37] [Epoch=204/300] [Need: 01:29:06] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[204/300]], [2018-03-17 17:44:37], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [204][000/127]   Time 0.308 (0.308)   Data 0.272 (0.272)   Loss 0.3724 (0.3724)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 17:44:38]
  **Train** Prec@1 85.414 Prec@5 99.009 Error@1 14.586
  **VAL** Prec@1 94.522 Prec@5 100.000 Error@1 5.478

==>>[2018-03-17 17:45:33] [Epoch=205/300] [Need: 01:28:10] [learning_rate=0.0000] [Best : Accuracy=95.08, Error=4.92]

==>>Epoch=[205/300]], [2018-03-17 17:45:33], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [205][000/127]   Time 0.293 (0.293)   Data 0.256 (0.256)   Loss 0.4036 (0.4036)   Prec@1 84.375 (84.375)   Prec@5 100.000 (100.000)   [2018-03-17 17:45:33]
  **Train** Prec@1 85.042 Prec@5 98.613 Error@1 14.958
  **VAL** Prec@1 95.225 Prec@5 100.000 Error@1 4.775

==>>[2018-03-17 17:46:34] [Epoch=206/300] [Need: 01:27:14] [learning_rate=0.0000] [Best : Accuracy=95.22, Error=4.78]

==>>Epoch=[206/300]], [2018-03-17 17:46:34], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [206][000/127]   Time 0.367 (0.367)   Data 0.330 (0.330)   Loss 0.3381 (0.3381)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:46:34]
  **Train** Prec@1 85.488 Prec@5 98.440 Error@1 14.512
  **VAL** Prec@1 94.382 Prec@5 99.860 Error@1 5.618

==>>[2018-03-17 17:47:29] [Epoch=207/300] [Need: 01:26:21] [learning_rate=0.0000] [Best : Accuracy=95.22, Error=4.78]

==>>Epoch=[207/300]], [2018-03-17 17:47:29], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [207][000/127]   Time 0.374 (0.374)   Data 0.337 (0.337)   Loss 0.2701 (0.2701)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-03-17 17:47:30]
  **Train** Prec@1 85.711 Prec@5 98.861 Error@1 14.289
  **VAL** Prec@1 94.242 Prec@5 99.860 Error@1 5.758

==>>[2018-03-17 17:48:25] [Epoch=208/300] [Need: 01:25:25] [learning_rate=0.0000] [Best : Accuracy=95.22, Error=4.78]

==>>Epoch=[208/300]], [2018-03-17 17:48:25], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [208][000/127]   Time 0.386 (0.386)   Data 0.351 (0.351)   Loss 0.4132 (0.4132)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2018-03-17 17:48:26]
  **Train** Prec@1 85.315 Prec@5 98.886 Error@1 14.685
  **VAL** Prec@1 93.961 Prec@5 100.000 Error@1 6.039

==>>[2018-03-17 17:49:21] [Epoch=209/300] [Need: 01:24:30] [learning_rate=0.0000] [Best : Accuracy=95.22, Error=4.78]

==>>Epoch=[209/300]], [2018-03-17 17:49:21], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [209][000/127]   Time 0.712 (0.712)   Data 0.671 (0.671)   Loss 0.7705 (0.7705)   Prec@1 78.125 (78.125)   Prec@5 96.875 (96.875)   [2018-03-17 17:49:22]
  **Train** Prec@1 85.587 Prec@5 98.737 Error@1 14.413
  **VAL** Prec@1 93.820 Prec@5 100.000 Error@1 6.180

==>>[2018-03-17 17:50:18] [Epoch=210/300] [Need: 01:23:35] [learning_rate=0.0000] [Best : Accuracy=95.22, Error=4.78]

==>>Epoch=[210/300]], [2018-03-17 17:50:18], LR=[0.0002], Batch=[32] [Model=SimpleNet]
  Epoch: [210][000/127]   Time 0.247 (0.247)   Data 0.210 (0.210)   Loss 0.4404 (0.4404)   Prec@1 81.250 (81.250)   Prec@5 100.000 (100.000)   [2018-03-17 17:50:18]
